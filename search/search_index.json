{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HNSW","text":"<p>HNSW is a high-performance Go implementation of the Hierarchical Navigable Small World (HNSW) algorithm for approximate nearest neighbor search in Go. It provides efficient similarity search capabilities for high-dimensional vector data with excellent performance characteristics.</p> <p>The core of HNSW was built by Coder</p>"},{"location":"#what-is-hnsw","title":"What is HNSW?","text":"<p>Hierarchical Navigable Small World (HNSW) is a graph-based algorithm for approximate nearest neighbor search. It creates a multi-layered graph structure that allows for fast and accurate similarity searches in high-dimensional spaces. The algorithm offers logarithmic complexity for search operations, making it suitable for large-scale applications.</p> <p>Learn more about the Graph Structure and Search Algorithms that make HNSW so efficient.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>High Performance: Optimized implementation with excellent search and insertion speeds</li> <li>Type Safety: Fully leverages Go's generics for type-safe operations</li> <li>Customizable: Configurable parameters to balance between search speed and accuracy</li> <li>Extensible: Modular design that allows for easy extensions and customizations</li> <li>Production Ready: Thoroughly tested and benchmarked for production use</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>HNSW is ideal for a wide range of applications that require similarity search:</p> <ul> <li>Recommendation Systems: Find similar products, articles, or content</li> <li>Image Search: Locate visually similar images</li> <li>Natural Language Processing: Semantic search and document similarity</li> <li>Anomaly Detection: Identify outliers in high-dimensional data</li> <li>Clustering: Group similar data points together</li> </ul> <p>Check out our Examples for practical implementations of these use cases.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/TFMV/hnsw\"\n)\n\nfunc main() {\n    // Create a new HNSW graph\n    graph := hnsw.NewGraph[int]()\n\n    // Add some vectors\n    graph.Add(hnsw.Node[int]{\n        Key:   1,\n        Value: []float32{0.1, 0.2, 0.3},\n    })\n\n    graph.Add(hnsw.Node[int]{\n        Key:   2,\n        Value: []float32{0.2, 0.3, 0.4},\n    })\n\n    // Search for similar vectors\n    results, _ := graph.Search([]float32{0.15, 0.25, 0.35}, 2)\n\n    for _, result := range results {\n        fmt.Printf(\"Key: %d, Distance: %f\\n\", result.Key, result.Dist)\n    }\n}\n</code></pre> <p>For more detailed instructions, see our Quick Start Guide and Basic Usage documentation.</p>"},{"location":"#core-library","title":"Core Library","text":"<p>The core library provides the fundamental functionality of HNSW:</p> <ul> <li>Graph Structure: Learn about the hierarchical structure of HNSW</li> <li>Distance Functions: Understand the different distance metrics available</li> <li>Search Algorithms: Explore the efficient search algorithms used</li> <li>Performance Tuning: Optimize HNSW for your specific use case</li> </ul>"},{"location":"#extensions","title":"Extensions","text":"<p>HNSW provides several extensions to enhance its functionality:</p> <ul> <li>Metadata Extension: Store and retrieve JSON metadata alongside vectors</li> <li>Faceted Search: Filter search results based on facets or attributes</li> <li>Creating Extensions: Build your own custom extensions</li> </ul> <p>See the Extensions Overview for more information.</p>"},{"location":"#performance","title":"Performance","text":"<p>HNSW is designed for high performance, with careful attention to memory usage and computational efficiency. Benchmarks show that it can handle millions of high-dimensional vectors with sub-millisecond query times.</p> <p>Learn more about Performance Tuning to get the most out of HNSW.</p>"},{"location":"#license","title":"License","text":"<p>HNSW is released under the Creative Commons Legal Code.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides a comprehensive reference for the HNSW API.</p>"},{"location":"api-reference/#core-types","title":"Core Types","text":""},{"location":"api-reference/#vector","title":"Vector","text":"<pre><code>type Vector = []float32\n</code></pre> <p>A <code>Vector</code> is a slice of float32 values representing a point in a high-dimensional space.</p>"},{"location":"api-reference/#node","title":"Node","text":"<pre><code>type Node[K cmp.Ordered] struct {\n    Key   K\n    Value Vector\n}\n</code></pre> <p>A <code>Node</code> represents a point in the vector space with a unique key.</p>"},{"location":"api-reference/#methods","title":"Methods","text":"<pre><code>func MakeNode[K cmp.Ordered](key K, vec Vector) Node[K]\n</code></pre> <p>Creates a new node with the specified key and vector.</p>"},{"location":"api-reference/#graph","title":"Graph","text":"<pre><code>type Graph[K cmp.Ordered] struct {\n    // Distance is the distance function used to compare embeddings.\n    Distance DistanceFunc\n\n    // M is the maximum number of neighbors to keep for each node.\n    // A good default for OpenAI embeddings is 16.\n    M int\n\n    // Ml is the level generation factor.\n    // E.g., for Ml = 0.25, each layer is 1/4 the size of the previous layer.\n    Ml float64\n\n    // EfSearch is the number of nodes to consider in the search phase.\n    // 20 is a reasonable default. Higher values improve search accuracy at\n    // the expense of memory.\n    EfSearch int\n}\n</code></pre> <p>A <code>Graph</code> is a Hierarchical Navigable Small World graph that stores vectors and allows for efficient similarity search.</p>"},{"location":"api-reference/#constructor-functions","title":"Constructor Functions","text":""},{"location":"api-reference/#newgraph","title":"NewGraph","text":"<pre><code>func NewGraph[K cmp.Ordered]() *Graph[K]\n</code></pre> <p>Creates a new graph with default parameters:</p> <ul> <li>M: 16</li> <li>Ml: 0.25</li> <li>Distance: CosineDistance</li> <li>EfSearch: 20</li> </ul>"},{"location":"api-reference/#newgraphwithconfig","title":"NewGraphWithConfig","text":"<pre><code>func NewGraphWithConfig[K cmp.Ordered](m int, ml float64, efSearch int, distance DistanceFunc) (*Graph[K], error)\n</code></pre> <p>Creates a new graph with the specified parameters and validates the configuration.</p>"},{"location":"api-reference/#graph-methods","title":"Graph Methods","text":""},{"location":"api-reference/#add","title":"Add","text":"<pre><code>func (g *Graph[K]) Add(nodes ...Node[K]) error\n</code></pre> <p>Adds one or more nodes to the graph. If a node with the same key already exists, it is replaced.</p>"},{"location":"api-reference/#batchadd","title":"BatchAdd","text":"<pre><code>func (g *Graph[K]) BatchAdd(nodes []Node[K]) error\n</code></pre> <p>Adds multiple nodes to the graph in a single operation. This is more efficient than calling <code>Add</code> multiple times when adding many nodes.</p>"},{"location":"api-reference/#search","title":"Search","text":"<pre><code>func (g *Graph[K]) Search(near Vector, k int) ([]Node[K], error)\n</code></pre> <p>Searches for the k nearest neighbors to the query vector.</p>"},{"location":"api-reference/#parallelsearch","title":"ParallelSearch","text":"<pre><code>func (g *Graph[K]) ParallelSearch(near Vector, k int, numWorkers int) ([]Node[K], error)\n</code></pre> <p>Performs a parallel search for the k nearest neighbors to the query vector using the specified number of worker goroutines.</p>"},{"location":"api-reference/#searchwithnegative","title":"SearchWithNegative","text":"<pre><code>func (g *Graph[K]) SearchWithNegative(near Vector, negative Vector, k int, negWeight float32) ([]Node[K], error)\n</code></pre> <p>Searches for vectors that are similar to the positive example (<code>near</code>) but dissimilar to the negative example (<code>negative</code>).</p>"},{"location":"api-reference/#searchwithnegatives","title":"SearchWithNegatives","text":"<pre><code>func (g *Graph[K]) SearchWithNegatives(near Vector, negatives []Vector, k int, negWeight float32) ([]Node[K], error)\n</code></pre> <p>Searches for vectors that are similar to the positive example (<code>near</code>) but dissimilar to multiple negative examples (<code>negatives</code>).</p>"},{"location":"api-reference/#delete","title":"Delete","text":"<pre><code>func (g *Graph[K]) Delete(key K) bool\n</code></pre> <p>Deletes a node from the graph. Returns true if the node was found and deleted.</p>"},{"location":"api-reference/#batchdelete","title":"BatchDelete","text":"<pre><code>func (g *Graph[K]) BatchDelete(keys []K) []bool\n</code></pre> <p>Deletes multiple nodes from the graph in a single operation. Returns a slice of booleans indicating which nodes were found and deleted.</p>"},{"location":"api-reference/#lookup","title":"Lookup","text":"<pre><code>func (g *Graph[K]) Lookup(key K) (Vector, bool)\n</code></pre> <p>Looks up a node by key and returns its vector. Returns false if the node is not found.</p>"},{"location":"api-reference/#len","title":"Len","text":"<pre><code>func (g *Graph[K]) Len() int\n</code></pre> <p>Returns the number of nodes in the graph.</p>"},{"location":"api-reference/#export","title":"Export","text":"<pre><code>func (g *Graph[K]) Export(w io.Writer) error\n</code></pre> <p>Exports the graph to the specified writer.</p>"},{"location":"api-reference/#import","title":"Import","text":"<pre><code>func (g *Graph[K]) Import(r io.Reader) error\n</code></pre> <p>Imports a graph from the specified reader.</p>"},{"location":"api-reference/#distance-functions","title":"Distance Functions","text":"<pre><code>type DistanceFunc func(a, b Vector) float32\n</code></pre> <p>A <code>DistanceFunc</code> calculates the distance between two vectors.</p>"},{"location":"api-reference/#built-in-distance-functions","title":"Built-in Distance Functions","text":""},{"location":"api-reference/#cosinedistance","title":"CosineDistance","text":"<pre><code>func CosineDistance(a, b Vector) float32\n</code></pre> <p>Calculates the cosine distance between two vectors: 1 - cosine similarity.</p>"},{"location":"api-reference/#euclideandistance","title":"EuclideanDistance","text":"<pre><code>func EuclideanDistance(a, b Vector) float32\n</code></pre> <p>Calculates the Euclidean (L2) distance between two vectors.</p>"},{"location":"api-reference/#dotproductdistance","title":"DotProductDistance","text":"<pre><code>func DotProductDistance(a, b Vector) float32\n</code></pre> <p>Calculates the negative dot product between two vectors.</p>"},{"location":"api-reference/#extensions","title":"Extensions","text":"<p>HNSW provides several extensions that enhance its functionality:</p>"},{"location":"api-reference/#metadata-extension","title":"Metadata Extension","text":"<p>The Metadata Extension allows you to store and retrieve JSON metadata alongside vectors in the graph.</p> <pre><code>import \"github.com/TFMV/hnsw/hnsw-extensions/meta\"\n</code></pre>"},{"location":"api-reference/#key-types","title":"Key Types","text":"<pre><code>type MetadataNode[K cmp.Ordered] struct {\n    Node     hnsw.Node[K]\n    Metadata json.RawMessage\n}\n\ntype MetadataGraph[K cmp.Ordered] struct {\n    Graph *hnsw.Graph[K]\n    Store MetadataStore[K]\n}\n</code></pre>"},{"location":"api-reference/#constructor","title":"Constructor","text":"<pre><code>func NewMetadataGraph[K cmp.Ordered](graph *hnsw.Graph[K], store MetadataStore[K]) *MetadataGraph[K]\n</code></pre> <p>Creates a new metadata graph that wraps an HNSW graph and a metadata store.</p>"},{"location":"api-reference/#faceted-search-extension","title":"Faceted Search Extension","text":"<p>The Faceted Search extension allows you to filter search results based on facets or attributes.</p> <pre><code>import \"github.com/TFMV/hnsw/hnsw-extensions/facets\"\n</code></pre>"},{"location":"api-reference/#key-types_1","title":"Key Types","text":"<pre><code>type FacetedNode[K cmp.Ordered] struct {\n    Node   hnsw.Node[K]\n    Facets map[string]interface{}\n}\n\ntype FacetedGraph[K cmp.Ordered] struct {\n    Graph *hnsw.Graph[K]\n    Store FacetStore[K]\n}\n</code></pre>"},{"location":"api-reference/#constructor_1","title":"Constructor","text":"<pre><code>func NewFacetedGraph[K cmp.Ordered](graph *hnsw.Graph[K], store FacetStore[K]) *FacetedGraph[K]\n</code></pre> <p>Creates a new faceted graph that wraps an HNSW graph and a facet store.</p>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":"<p>Most methods in the HNSW API return errors that should be checked. Common errors include:</p> <ul> <li>Dimension mismatch errors when adding vectors with different dimensions</li> <li>Configuration validation errors when creating a graph with invalid parameters</li> <li>I/O errors when importing or exporting graphs</li> </ul> <p>Example error handling:</p> <pre><code>err := graph.Add(node)\nif err != nil {\n    // Handle error\n    log.Fatalf(\"Failed to add node: %v\", err)\n}\n</code></pre>"},{"location":"core/distance-functions/","title":"Distance Functions","text":"<p>Distance functions are a critical component of the HNSW algorithm, as they determine how similarity between vectors is measured. This page explains the different distance metrics available in HNSW and how to choose the right one for your use case.</p>"},{"location":"core/distance-functions/#overview","title":"Overview","text":"<p>In HNSW, distance functions are used to:</p> <ol> <li>Determine the nearest neighbors during graph construction</li> <li>Find the closest vectors during search operations</li> <li>Calculate the distance between vectors for ranking search results</li> </ol> <p>The choice of distance function can significantly impact the quality of search results and should be selected based on the properties of your vector data.</p>"},{"location":"core/distance-functions/#built-in-distance-functions","title":"Built-in Distance Functions","text":"<p>HNSW provides several built-in distance functions:</p>"},{"location":"core/distance-functions/#cosine-distance","title":"Cosine Distance","text":"<pre><code>func CosineDistance(a, b Vector) float32\n</code></pre> <p>Cosine distance measures the cosine of the angle between two vectors. It is defined as:</p> <pre><code>CosineDistance(a, b) = 1 - (a\u00b7b) / (||a|| * ||b||)\n</code></pre> <p>Where:</p> <ul> <li><code>a\u00b7b</code> is the dot product of vectors a and b</li> <li><code>||a||</code> and <code>||b||</code> are the magnitudes (Euclidean norms) of vectors a and b</li> </ul> <p>Properties:</p> <ul> <li>Range: [0, 2] (0 means identical, 2 means opposite directions)</li> <li>Invariant to vector scaling (only direction matters)</li> <li>Best for normalized vectors (e.g., embeddings from language models)</li> </ul> <p>Use cases:</p> <ul> <li>Text embeddings (e.g., from OpenAI, BERT, etc.)</li> <li>Semantic search</li> <li>Recommendation systems</li> </ul> <p>Implementation:</p> <pre><code>func CosineDistance(a, b Vector) float32 {\n    var (\n        dot    float32\n        normA  float32\n        normB  float32\n    )\n\n    for i := range a {\n        dot += a[i] * b[i]\n        normA += a[i] * a[i]\n        normB += b[i] * b[i]\n    }\n\n    if normA == 0 || normB == 0 {\n        return 1.0\n    }\n\n    return 1.0 - dot/float32(math.Sqrt(float64(normA)*float64(normB)))\n}\n</code></pre>"},{"location":"core/distance-functions/#euclidean-distance","title":"Euclidean Distance","text":"<pre><code>func EuclideanDistance(a, b Vector) float32\n</code></pre> <p>Euclidean distance measures the straight-line distance between two points in Euclidean space. It is defined as:</p> <pre><code>EuclideanDistance(a, b) = sqrt(sum((a_i - b_i)\u00b2))\n</code></pre> <p>Properties:</p> <ul> <li>Range: [0, \u221e) (0 means identical)</li> <li>Sensitive to vector scaling</li> <li>Preserves the geometric interpretation of distance</li> </ul> <p>Use cases:</p> <ul> <li>Image feature vectors</li> <li>Geographical coordinates</li> <li>Physical measurements</li> <li>Any application where the absolute magnitude matters</li> </ul> <p>Implementation:</p> <pre><code>func EuclideanDistance(a, b Vector) float32 {\n    var sum float32\n\n    for i := range a {\n        d := a[i] - b[i]\n        sum += d * d\n    }\n\n    return float32(math.Sqrt(float64(sum)))\n}\n</code></pre>"},{"location":"core/distance-functions/#dot-product-distance","title":"Dot Product Distance","text":"<pre><code>func DotProductDistance(a, b Vector) float32\n</code></pre> <p>Dot product distance is the negative of the dot product between two vectors. It is defined as:</p> <pre><code>DotProductDistance(a, b) = -sum(a_i * b_i)\n</code></pre> <p>Properties:</p> <ul> <li>Range: (-\u221e, \u221e)</li> <li>Lower values indicate higher similarity</li> <li>Sensitive to vector magnitude</li> </ul> <p>Use cases:</p> <ul> <li>When vectors are already normalized</li> <li>Specialized applications where dot product is the natural similarity measure</li> </ul> <p>Implementation:</p> <pre><code>func DotProductDistance(a, b Vector) float32 {\n    var dot float32\n\n    for i := range a {\n        dot += a[i] * b[i]\n    }\n\n    return -dot\n}\n</code></pre>"},{"location":"core/distance-functions/#custom-distance-functions","title":"Custom Distance Functions","text":"<p>You can define your own distance function to suit your specific needs:</p> <pre><code>graph := hnsw.NewGraph[int]()\n\n// Define a custom distance function\ngraph.Distance = func(a, b []float32) float32 {\n    // Manhattan distance (L1 norm)\n    var sum float32\n    for i := range a {\n        sum += float32(math.Abs(float64(a[i] - b[i])))\n    }\n    return sum\n}\n</code></pre> <p>Common custom distance functions include:</p>"},{"location":"core/distance-functions/#manhattan-distance-l1-norm","title":"Manhattan Distance (L1 Norm)","text":"<pre><code>func ManhattanDistance(a, b Vector) float32 {\n    var sum float32\n\n    for i := range a {\n        sum += float32(math.Abs(float64(a[i] - b[i])))\n    }\n\n    return sum\n}\n</code></pre>"},{"location":"core/distance-functions/#chebyshev-distance-l-norm","title":"Chebyshev Distance (L\u221e Norm)","text":"<pre><code>func ChebyshevDistance(a, b Vector) float32 {\n    var max float32\n\n    for i := range a {\n        d := float32(math.Abs(float64(a[i] - b[i])))\n        if d &gt; max {\n            max = d\n        }\n    }\n\n    return max\n}\n</code></pre>"},{"location":"core/distance-functions/#weighted-euclidean-distance","title":"Weighted Euclidean Distance","text":"<pre><code>func WeightedEuclideanDistance(weights Vector) DistanceFunc {\n    return func(a, b Vector) float32 {\n        var sum float32\n\n        for i := range a {\n            d := a[i] - b[i]\n            sum += weights[i] * d * d\n        }\n\n        return float32(math.Sqrt(float64(sum)))\n    }\n}\n</code></pre>"},{"location":"core/distance-functions/#choosing-the-right-distance-function","title":"Choosing the Right Distance Function","text":"<p>The choice of distance function depends on the properties of your data and the requirements of your application:</p> <ol> <li>For text embeddings and semantic search:</li> <li>Use Cosine Distance if vectors are normalized</li> <li> <p>Use Dot Product Distance if vectors are already normalized</p> </li> <li> <p>For image features and physical measurements:</p> </li> <li>Use Euclidean Distance if the absolute magnitude matters</li> <li> <p>Use Manhattan Distance if you want to reduce the influence of outliers</p> </li> <li> <p>For geographical coordinates:</p> </li> <li>Use Euclidean Distance for small areas</li> <li> <p>Use a specialized geodesic distance for large areas</p> </li> <li> <p>For mixed data types:</p> </li> <li>Use a weighted combination of distance functions</li> <li>Consider normalizing each feature before computing the distance</li> </ol>"},{"location":"core/distance-functions/#performance-considerations","title":"Performance Considerations","text":"<p>Distance calculations are the most computationally intensive part of the HNSW algorithm, as they are performed many times during both construction and search. Here are some tips for optimizing distance calculations:</p> <ol> <li>Precompute norms for cosine distance:</li> <li>If using cosine distance, consider precomputing and storing the norms of vectors</li> <li> <p>This can significantly reduce the computation time during search</p> </li> <li> <p>Use SIMD instructions:</p> </li> <li>Modern CPUs support SIMD (Single Instruction, Multiple Data) instructions</li> <li>These can accelerate vector operations like dot products and Euclidean distance</li> <li> <p>The Go compiler may automatically use SIMD instructions for some operations</p> </li> <li> <p>Consider dimensionality reduction:</p> </li> <li>High-dimensional vectors require more computation for distance calculations</li> <li>Consider using dimensionality reduction techniques like PCA or t-SNE</li> <li> <p>This can improve both performance and memory usage</p> </li> <li> <p>Normalize vectors:</p> </li> <li>For cosine distance, normalizing vectors in advance can simplify calculations</li> <li>This is especially useful for text embeddings from language models</li> </ol>"},{"location":"core/distance-functions/#next-steps","title":"Next Steps","text":"<p>Now that you understand distance functions in HNSW, you can explore:</p> <ul> <li>Graph Structure: Learn about the hierarchical structure of HNSW</li> <li>Search Algorithms: Dive deeper into the search algorithms used in HNSW</li> <li>Performance Tuning: Learn how to optimize HNSW for your specific use case</li> </ul>"},{"location":"core/graph-structure/","title":"Graph Structure","text":"<p>This page provides a comprehensive overview of the HNSW graph structure and architecture.</p>"},{"location":"core/graph-structure/#hnsw-architecture-overview","title":"HNSW Architecture Overview","text":"<p>HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor search. Its architecture consists of multiple layers of graphs, where each layer is a navigable small world graph.</p> <p>The key components of the HNSW architecture include:</p> <ol> <li>Hierarchical Structure: Multiple layers of graphs with decreasing density</li> <li>Small World Graphs: Each layer is a navigable small world graph</li> <li>Greedy Search: Efficient search algorithm that traverses the layers</li> </ol> <p>This multi-layered approach allows HNSW to achieve logarithmic search complexity, making it one of the fastest algorithms for approximate nearest neighbor search.</p>"},{"location":"core/graph-structure/#hierarchical-structure","title":"Hierarchical Structure","text":"<p>The hierarchical structure of HNSW consists of multiple layers (L0, L1, L2, ..., Lmax), where:</p> <ul> <li>L0 is the bottom layer containing all nodes</li> <li>Each subsequent layer contains a subset of nodes from the layer below</li> <li>The top layer (Lmax) contains very few nodes</li> </ul> <p>The probability of a node being promoted to a higher layer follows a geometric distribution, resulting in an exponential decrease in the number of nodes as we move up the hierarchy:</p> <ul> <li>Layer L0: 100% of nodes</li> <li>Layer L1: ~1/M nodes from L0</li> <li>Layer L2: ~1/M nodes from L1</li> <li>And so on...</li> </ul> <p>Where M is a parameter that controls the maximum number of connections per node.</p> <p>This hierarchical structure enables efficient search by allowing the algorithm to quickly navigate to the approximate region of the query point in the higher layers, then refine the search in the lower layers.</p>"},{"location":"core/graph-structure/#small-world-graphs","title":"Small World Graphs","text":"<p>Each layer in HNSW is a navigable small world graph, which has the following properties:</p> <ol> <li>Short Path Length: The average path length between any two nodes is logarithmic in the number of nodes</li> <li>High Clustering: Nodes tend to form clusters with their nearest neighbors</li> <li>Long-Range Connections: Some connections span large distances, enabling efficient navigation</li> </ol> <p>These properties allow for efficient navigation through the graph, as the algorithm can make large jumps using long-range connections and then refine the search using local connections.</p>"},{"location":"core/graph-structure/#graph-construction","title":"Graph Construction","text":"<p>When adding a new vector to the HNSW graph, the following steps are performed:</p> <ol> <li>Layer Assignment: Randomly assign the new node to layers based on a geometric distribution</li> <li>Entry Point Selection: Start from the entry point at the top layer</li> <li>Greedy Search: Find the closest node to the new vector at each layer</li> <li>Connection Establishment: Connect the new node to its nearest neighbors at each assigned layer</li> <li>Connection Optimization: Ensure that connections satisfy the small world properties</li> </ol> <p>The graph construction process ensures that each node is connected to its nearest neighbors while maintaining the navigable small world properties.</p>"},{"location":"core/graph-structure/#search-algorithm","title":"Search Algorithm","text":"<p>The search algorithm in HNSW follows these steps:</p> <ol> <li>Start at the Entry Point: Begin the search from the entry point at the top layer</li> <li>Greedy Search at Each Layer: At each layer, perform a greedy search to find the closest node to the query</li> <li>Layer Transition: Use the closest node found as the entry point for the next layer</li> <li>Final Search: At the bottom layer, perform a more thorough search to find the k nearest neighbors</li> </ol> <p>This approach allows HNSW to achieve logarithmic time complexity for search operations, making it highly efficient for large datasets.</p>"},{"location":"core/graph-structure/#implementation-details","title":"Implementation Details","text":""},{"location":"core/graph-structure/#data-structures","title":"Data Structures","text":"<p>The HNSW graph is implemented using the following data structures:</p> <pre><code>type Graph[T comparable] struct {\n    nodes       map[T]*Node[T]\n    vectors     map[T][]float32\n    entryPoint  T\n    maxLayer    int\n    M           int  // Maximum number of connections per node\n    efConstruction int  // Size of the dynamic candidate list during construction\n    Distance    DistanceFunc\n    // ... other fields\n}\n\ntype Node[T comparable] struct {\n    ID        T\n    Neighbors map[int][]T  // Map from layer to neighbor IDs\n    // ... other fields\n}\n\ntype DistanceFunc func(a, b []float32) float32\n</code></pre> <p>The graph stores nodes and their vectors separately, with connections represented as maps from layer to neighbor IDs.</p>"},{"location":"core/graph-structure/#memory-layout","title":"Memory Layout","text":"<p>The memory layout of the HNSW graph is optimized for efficient access and cache locality:</p> <ol> <li>Nodes: Stored in a map for O(1) access by ID</li> <li>Vectors: Stored in a separate map to allow for different storage strategies</li> <li>Connections: Stored as adjacency lists for each node at each layer</li> </ol> <p>This layout allows for efficient traversal of the graph during search operations.</p>"},{"location":"core/graph-structure/#performance-characteristics","title":"Performance Characteristics","text":"<p>The HNSW algorithm has the following performance characteristics:</p> <ul> <li>Construction Time: O(n log n) for n vectors</li> <li>Search Time: O(log n) for finding the nearest neighbor</li> <li>Memory Usage: O(n M), where M is the maximum number of connections per node</li> </ul> <p>These characteristics make HNSW suitable for large-scale nearest neighbor search applications.</p>"},{"location":"core/graph-structure/#concurrency-and-thread-safety","title":"Concurrency and Thread Safety","text":"<p>The HNSW implementation supports concurrent operations with the following characteristics:</p> <ul> <li>Read Operations: Multiple search operations can be performed concurrently</li> <li>Write Operations: Adding nodes requires exclusive access to the graph</li> <li>Mixed Operations: Read operations can proceed concurrently with write operations with proper synchronization</li> </ul> <p>This allows for efficient utilization of multi-core processors in high-throughput applications.</p>"},{"location":"core/graph-structure/#next-steps","title":"Next Steps","text":"<p>Now that you understand the graph structure of HNSW, you can explore:</p> <ul> <li>Distance Functions: Learn about the different distance metrics available in HNSW</li> <li>Search Algorithms: Dive deeper into the search algorithms used in HNSW</li> <li>Performance Tuning: Learn how to optimize HNSW for your specific use case</li> </ul>"},{"location":"core/performance-tuning/","title":"Performance Tuning","text":"<p>This page provides guidance on how to optimize HNSW for your specific use case, balancing search speed, memory usage, and accuracy.</p>"},{"location":"core/performance-tuning/#overview","title":"Overview","text":"<p>HNSW performance is influenced by several factors:</p> <ol> <li>Graph construction parameters - Control the structure and connectivity of the graph</li> <li>Search parameters - Determine the trade-off between search speed and accuracy</li> <li>Hardware considerations - CPU, memory, and cache utilization</li> <li>Data characteristics - Dimensionality, distribution, and size of the dataset</li> </ol> <p>By tuning these parameters, you can optimize HNSW for your specific requirements.</p>"},{"location":"core/performance-tuning/#key-performance-parameters","title":"Key Performance Parameters","text":""},{"location":"core/performance-tuning/#m-maximum-number-of-connections","title":"M (Maximum Number of Connections)","text":"<p>The <code>M</code> parameter controls the maximum number of connections per node in the graph:</p> <pre><code>graph := hnsw.NewGraph[int](hnsw.WithM(16))\n</code></pre> <ul> <li>Higher M values (e.g., 32-64):</li> <li>Improve search accuracy</li> <li>Increase memory usage</li> <li>Slow down graph construction</li> <li> <p>Potentially improve search speed (up to a point)</p> </li> <li> <p>Lower M values (e.g., 5-10):</p> </li> <li>Reduce memory usage</li> <li>Speed up graph construction</li> <li>May reduce search accuracy</li> </ul> <p>Recommended values:</p> <ul> <li>For high-dimensional data (&gt;100 dimensions): 16-32</li> <li>For medium-dimensional data (20-100 dimensions): 12-16</li> <li>For low-dimensional data (&lt;20 dimensions): 5-10</li> </ul>"},{"location":"core/performance-tuning/#ef-construction","title":"ef Construction","text":"<p>The <code>efConstruction</code> parameter controls the size of the dynamic candidate list during graph construction:</p> <pre><code>graph := hnsw.NewGraph[int](hnsw.WithEfConstruction(200))\n</code></pre> <ul> <li>Higher efConstruction values (e.g., 200-500):</li> <li>Improve graph quality and search accuracy</li> <li>Slow down graph construction</li> <li> <p>No impact on search speed or memory usage after construction</p> </li> <li> <p>Lower efConstruction values (e.g., 40-100):</p> </li> <li>Speed up graph construction</li> <li>May reduce search accuracy</li> </ul> <p>Recommended values:</p> <ul> <li>For high recall requirements (&gt;95%): 200-500</li> <li>For balanced performance: 100-200</li> <li>For fast construction with acceptable accuracy: 40-100</li> </ul>"},{"location":"core/performance-tuning/#ef-search","title":"ef Search","text":"<p>The <code>ef</code> parameter controls the size of the dynamic candidate list during search:</p> <pre><code>results := graph.SearchWithEF(query, 10, 100)\n</code></pre> <ul> <li>Higher ef values (e.g., 100-1000):</li> <li>Improve search accuracy (recall)</li> <li> <p>Slow down search</p> </li> <li> <p>Lower ef values (e.g., 10-50):</p> </li> <li>Speed up search</li> <li>May reduce search accuracy</li> </ul> <p>Recommended values:</p> <ul> <li>For high recall requirements (&gt;95%): ef = 100-200</li> <li>For balanced performance: ef = 50-100</li> <li>For fast search with acceptable accuracy: ef = 10-50</li> </ul>"},{"location":"core/performance-tuning/#batch-size","title":"Batch Size","text":"<p>When adding multiple vectors, the batch size can impact performance:</p> <pre><code>// Add vectors in batches\nfor i := 0; i &lt; len(vectors); i += batchSize {\n    end := i + batchSize\n    if end &gt; len(vectors) {\n        end = len(vectors)\n    }\n    graph.BatchAdd(vectors[i:end], ids[i:end])\n}\n</code></pre> <ul> <li>Larger batch sizes (e.g., 1000-10000):</li> <li>Better memory locality and CPU cache utilization</li> <li>More efficient parallel processing</li> <li> <p>Higher memory usage during batch processing</p> </li> <li> <p>Smaller batch sizes (e.g., 100-1000):</p> </li> <li>Lower memory usage</li> <li>Potentially slower overall construction</li> </ul> <p>Recommended values:</p> <ul> <li>For most systems: 1000-5000 vectors per batch</li> <li>For memory-constrained systems: 100-1000 vectors per batch</li> </ul>"},{"location":"core/performance-tuning/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>To find the optimal parameters for your specific use case, it's recommended to run benchmarks:</p> <pre><code>func benchmarkSearch(graph *hnsw.Graph, queries []Vector, k int, efValues []int) {\n    for _, ef := range efValues {\n        startTime := time.Now()\n\n        var results [][]Node\n        for _, query := range queries {\n            results = append(results, graph.SearchWithEF(query, k, ef))\n        }\n\n        duration := time.Since(startTime)\n        avgTime := duration.Seconds() / float64(len(queries))\n\n        // Calculate recall if ground truth is available\n        recall := calculateRecall(results, groundTruth)\n\n        fmt.Printf(\"ef=%d: avg_time=%.6fs, recall=%.4f\\n\", ef, avgTime, recall)\n    }\n}\n</code></pre> <p>This benchmark helps you find the optimal <code>ef</code> value for your requirements.</p>"},{"location":"core/performance-tuning/#memory-optimization","title":"Memory Optimization","text":""},{"location":"core/performance-tuning/#compact-storage","title":"Compact Storage","text":"<p>HNSW can use a significant amount of memory, especially for large datasets. To reduce memory usage:</p> <pre><code>// Use compact storage for vectors\ngraph := hnsw.NewGraph[int](hnsw.WithCompactVectors(true))\n</code></pre> <p>Compact storage reduces memory usage by:</p> <ul> <li>Storing vectors more efficiently</li> <li>Using less memory for graph connectivity</li> <li>Potentially improving cache locality</li> </ul>"},{"location":"core/performance-tuning/#quantization","title":"Quantization","text":"<p>For very large datasets, you can use vector quantization to reduce memory usage:</p> <pre><code>// Use 8-bit quantization for vectors\ngraph := hnsw.NewGraph[int](hnsw.WithQuantization(8))\n</code></pre> <p>Quantization reduces memory usage by:</p> <ul> <li>Converting 32-bit floats to lower precision (e.g., 8-bit integers)</li> <li>Potentially reducing search accuracy</li> <li>Improving cache locality and search speed</li> </ul>"},{"location":"core/performance-tuning/#external-storage","title":"External Storage","text":"<p>For extremely large datasets that don't fit in memory, you can use external storage:</p> <pre><code>// Use memory-mapped storage for vectors\nstorage := hnsw.NewMMapVectorStorage(\"vectors.bin\")\ngraph := hnsw.NewGraph[int](hnsw.WithExternalVectorStorage(storage))\n</code></pre> <p>External storage:</p> <ul> <li>Allows datasets larger than available RAM</li> <li>May slow down search due to disk I/O</li> <li>Can be combined with caching strategies for frequently accessed vectors</li> </ul>"},{"location":"core/performance-tuning/#cpu-optimization","title":"CPU Optimization","text":""},{"location":"core/performance-tuning/#simd-acceleration","title":"SIMD Acceleration","text":"<p>HNSW can benefit from SIMD (Single Instruction, Multiple Data) instructions for distance calculations:</p> <pre><code>// Enable SIMD acceleration for distance calculations\ngraph := hnsw.NewGraph[int](hnsw.WithSIMD(true))\n</code></pre> <p>SIMD acceleration:</p> <ul> <li>Speeds up distance calculations by processing multiple vector elements in parallel</li> <li>Works best for Euclidean and cosine distance</li> <li>May not be available on all platforms</li> </ul>"},{"location":"core/performance-tuning/#thread-pool","title":"Thread Pool","text":"<p>For concurrent operations, using a thread pool can improve performance:</p> <pre><code>// Use a thread pool with 8 workers\npool := hnsw.NewThreadPool(8)\ngraph := hnsw.NewGraph[int](hnsw.WithThreadPool(pool))\n</code></pre> <p>A thread pool:</p> <ul> <li>Reduces thread creation overhead</li> <li>Allows better control of parallelism</li> <li>Can be shared across multiple HNSW graphs</li> </ul>"},{"location":"core/performance-tuning/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"core/performance-tuning/#sharding","title":"Sharding","text":"<p>For very large datasets, you can use sharding to distribute the data across multiple HNSW graphs:</p> <pre><code>// Create multiple HNSW graphs (shards)\nshards := make([]*hnsw.Graph, numShards)\nfor i := range shards {\n    shards[i] = hnsw.NewGraph[int]()\n}\n\n// Add vectors to shards based on some partitioning strategy\nfor i, vector := range vectors {\n    shardIndex := getShardIndex(vector) // Custom sharding function\n    shards[shardIndex].Add(vector, ids[i])\n}\n\n// Search across all shards and merge results\nfunc searchAcrossShards(query Vector, k int) []Node {\n    var allResults []Node\n\n    // Search each shard concurrently\n    var wg sync.WaitGroup\n    resultChan := make(chan []Node, len(shards))\n\n    for _, shard := range shards {\n        wg.Add(1)\n        go func(shard *hnsw.Graph) {\n            defer wg.Done()\n            results := shard.Search(query, k)\n            resultChan &lt;- results\n        }(shard)\n    }\n\n    // Wait for all searches to complete\n    wg.Wait()\n    close(resultChan)\n\n    // Collect and merge results\n    for results := range resultChan {\n        allResults = append(allResults, results...)\n    }\n\n    // Sort and return top k\n    sort.Slice(allResults, func(i, j int) bool {\n        return allResults[i].Distance &lt; allResults[j].Distance\n    })\n\n    if len(allResults) &gt; k {\n        allResults = allResults[:k]\n    }\n\n    return allResults\n}\n</code></pre> <p>Sharding:</p> <ul> <li>Allows scaling beyond the memory limits of a single machine</li> <li>Enables parallel processing across shards</li> <li>May reduce search accuracy if not implemented carefully</li> </ul>"},{"location":"core/performance-tuning/#distributed-hnsw","title":"Distributed HNSW","text":"<p>For extremely large datasets, you can implement a distributed HNSW system:</p> <pre><code>// Example of a simple distributed HNSW client\ntype DistributedHNSW struct {\n    shardClients []*ShardClient\n}\n\nfunc (d *DistributedHNSW) Search(query Vector, k int) []Node {\n    var allResults []Node\n\n    // Search each shard concurrently\n    var wg sync.WaitGroup\n    resultChan := make(chan []Node, len(d.shardClients))\n\n    for _, client := range d.shardClients {\n        wg.Add(1)\n        go func(client *ShardClient) {\n            defer wg.Done()\n            results := client.Search(query, k)\n            resultChan &lt;- results\n        }(client)\n    }\n\n    // Wait for all searches to complete\n    wg.Wait()\n    close(resultChan)\n\n    // Collect and merge results\n    for results := range resultChan {\n        allResults = append(allResults, results...)\n    }\n\n    // Sort and return top k\n    sort.Slice(allResults, func(i, j int) bool {\n        return allResults[i].Distance &lt; allResults[j].Distance\n    })\n\n    if len(allResults) &gt; k {\n        allResults = allResults[:k]\n    }\n\n    return allResults\n}\n</code></pre> <p>Distributed HNSW:</p> <ul> <li>Scales to billions of vectors across multiple machines</li> <li>Requires careful design of the distribution strategy</li> <li>Introduces network latency for search operations</li> </ul>"},{"location":"core/performance-tuning/#real-world-performance-tuning-examples","title":"Real-world Performance Tuning Examples","text":""},{"location":"core/performance-tuning/#high-recall-text-search","title":"High-Recall Text Search","text":"<p>For applications requiring high recall, such as semantic text search:</p> <pre><code>// Configuration for high-recall text search\ngraph := hnsw.NewGraph[int](\n    hnsw.WithM(16),\n    hnsw.WithEfConstruction(300),\n    hnsw.WithDistance(hnsw.CosineDistance),\n)\n\n// Add vectors (text embeddings)\nfor i, embedding := range embeddings {\n    graph.Add(embedding, i)\n}\n\n// Search with high ef for better recall\nresults := graph.SearchWithEF(queryEmbedding, 10, 200)\n</code></pre>"},{"location":"core/performance-tuning/#fast-image-similarity-search","title":"Fast Image Similarity Search","text":"<p>For applications requiring fast search, such as real-time image similarity:</p> <pre><code>// Configuration for fast image similarity search\ngraph := hnsw.NewGraph[int](\n    hnsw.WithM(8),\n    hnsw.WithEfConstruction(100),\n    hnsw.WithDistance(hnsw.EuclideanDistance),\n    hnsw.WithSIMD(true),\n)\n\n// Add vectors (image features)\nfor i, features := range imageFeatures {\n    graph.Add(features, i)\n}\n\n// Search with low ef for faster results\nresults := graph.SearchWithEF(queryFeatures, 5, 20)\n</code></pre>"},{"location":"core/performance-tuning/#memory-efficient-recommendation-system","title":"Memory-Efficient Recommendation System","text":"<p>For large-scale recommendation systems with memory constraints:</p> <pre><code>// Configuration for memory-efficient recommendation system\ngraph := hnsw.NewGraph[int](\n    hnsw.WithM(10),\n    hnsw.WithEfConstruction(150),\n    hnsw.WithCompactVectors(true),\n    hnsw.WithQuantization(8),\n)\n\n// Add vectors in batches to manage memory usage\nbatchSize := 1000\nfor i := 0; i &lt; len(itemVectors); i += batchSize {\n    end := i + batchSize\n    if end &gt; len(itemVectors) {\n        end = len(itemVectors)\n    }\n    graph.BatchAdd(itemVectors[i:end], itemIDs[i:end])\n}\n\n// Search with moderate ef for balanced performance\nresults := graph.SearchWithEF(userPreferences, 20, 50)\n</code></pre>"},{"location":"core/performance-tuning/#monitoring-and-profiling","title":"Monitoring and Profiling","text":"<p>To continuously optimize HNSW performance, implement monitoring and profiling:</p> <pre><code>// Simple search profiling\nfunc profileSearch(graph *hnsw.Graph, query Vector, k int, ef int) {\n    startTime := time.Now()\n    results := graph.SearchWithEF(query, k, ef)\n    duration := time.Since(startTime)\n\n    fmt.Printf(\"Search stats: time=%.6fs, results=%d, ef=%d\\n\", \n               duration.Seconds(), len(results), ef)\n}\n\n// Memory usage monitoring\nfunc monitorMemoryUsage() {\n    var m runtime.MemStats\n    runtime.ReadMemStats(&amp;m)\n\n    fmt.Printf(\"Memory stats: alloc=%.2f MB, sys=%.2f MB, gc=%d\\n\",\n               float64(m.Alloc)/1024/1024,\n               float64(m.Sys)/1024/1024,\n               m.NumGC)\n}\n</code></pre> <p>Regular profiling helps identify performance bottlenecks and opportunities for optimization.</p>"},{"location":"core/performance-tuning/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to tune HNSW performance, you can explore:</p> <ul> <li>Graph Structure: Learn about the hierarchical structure of HNSW</li> <li>Search Algorithms: Dive deeper into the search algorithms used in HNSW</li> <li>Distance Functions: Understand how to choose the right distance function</li> </ul>"},{"location":"core/search-algorithms/","title":"Search Algorithms","text":"<p>This page explains the search algorithms used in HNSW, how they work, and how to optimize them for your specific use case.</p>"},{"location":"core/search-algorithms/#overview","title":"Overview","text":"<p>HNSW (Hierarchical Navigable Small World) uses a multi-layered graph structure to enable efficient approximate nearest neighbor search. The search algorithm is a key component that allows HNSW to achieve logarithmic time complexity for search operations.</p> <p>The core search algorithm in HNSW is a greedy search with a beam-like approach that:</p> <ol> <li>Starts at the top layer of the graph (with fewest nodes)</li> <li>Performs a greedy search to find the closest node to the query</li> <li>Uses this node as an entry point to the next layer</li> <li>Repeats the process until reaching the bottom layer</li> <li>Returns the closest nodes found in the bottom layer</li> </ol>"},{"location":"core/search-algorithms/#basic-search-algorithm","title":"Basic Search Algorithm","text":"<p>The basic search algorithm in HNSW is implemented as follows:</p> <pre><code>func (g *Graph) Search(query Vector, k int) []Node {\n    // Start from the entry point at the top layer\n    currentNode := g.entryPoint\n    currentDistance := g.Distance(query, currentNode.Vector)\n\n    // Traverse the layers from top to bottom\n    for layer := g.maxLayer; layer &gt; 0; layer-- {\n        // Greedy search in the current layer\n        changed := true\n        for changed {\n            changed = false\n\n            // Check all neighbors of the current node at this layer\n            for _, neighbor := range currentNode.Neighbors[layer] {\n                neighborDistance := g.Distance(query, neighbor.Vector)\n\n                // If we found a closer node, move to it\n                if neighborDistance &lt; currentDistance {\n                    currentNode = neighbor\n                    currentDistance = neighborDistance\n                    changed = true\n                    break\n                }\n            }\n        }\n    }\n\n    // Perform the final search in the bottom layer (layer 0)\n    // using a priority queue to keep track of the k closest nodes\n    candidates := NewPriorityQueue(k)\n    candidates.Push(currentNode, currentDistance)\n\n    visited := make(map[NodeID]bool)\n    visited[currentNode.ID] = true\n\n    for candidates.Len() &gt; 0 {\n        // Get the closest unvisited node\n        current, _ := candidates.Pop()\n\n        // Check all neighbors of the current node\n        for _, neighbor := range current.Neighbors[0] {\n            if visited[neighbor.ID] {\n                continue\n            }\n\n            visited[neighbor.ID] = true\n            neighborDistance := g.Distance(query, neighbor.Vector)\n\n            // If we haven't found k nodes yet, or if this node is closer than the furthest in our result\n            if candidates.Len() &lt; k || neighborDistance &lt; candidates.MaxDistance() {\n                candidates.Push(neighbor, neighborDistance)\n            }\n        }\n    }\n\n    // Return the k closest nodes\n    return candidates.Items()\n}\n</code></pre>"},{"location":"core/search-algorithms/#beam-search","title":"Beam Search","text":"<p>HNSW uses a beam search approach in the bottom layer to improve the quality of search results. Instead of a simple greedy search, it maintains a priority queue of the best candidates found so far and explores their neighborhoods.</p> <p>The beam search algorithm:</p> <ol> <li>Initializes a priority queue with the entry point</li> <li>Repeatedly extracts the closest node from the queue</li> <li>Explores its neighbors and adds promising ones to the queue</li> <li>Continues until the queue is empty or a stopping condition is met</li> </ol> <p>This approach helps avoid getting stuck in local minima and improves the recall of the search.</p>"},{"location":"core/search-algorithms/#search-with-ef-parameter","title":"Search with ef Parameter","text":"<p>The <code>ef</code> parameter controls the size of the dynamic candidate list during search. A larger <code>ef</code> value results in more accurate but slower searches:</p> <pre><code>func (g *Graph) SearchWithEF(query Vector, k int, ef int) []Node {\n    // Similar to the basic search, but with a larger candidate pool\n    // controlled by the ef parameter\n\n    // ... (code for traversing layers)\n\n    // Use a priority queue with capacity ef instead of k\n    candidates := NewPriorityQueue(ef)\n    // ... (rest of the search algorithm)\n}\n</code></pre> <p>The <code>ef</code> parameter allows you to trade off between search speed and accuracy:</p> <ul> <li>Small <code>ef</code> (e.g., 10-50): Faster searches but potentially lower recall</li> <li>Large <code>ef</code> (e.g., 100-1000): Slower searches but higher recall</li> </ul>"},{"location":"core/search-algorithms/#concurrent-search","title":"Concurrent Search","text":"<p>HNSW supports concurrent searches, which can significantly improve throughput when processing multiple queries:</p> <pre><code>func (g *Graph) ConcurrentSearch(queries []Vector, k int) [][]Node {\n    results := make([][]Node, len(queries))\n\n    var wg sync.WaitGroup\n    for i, query := range queries {\n        wg.Add(1)\n        go func(i int, query Vector) {\n            defer wg.Done()\n            results[i] = g.Search(query, k)\n        }(i, query)\n    }\n\n    wg.Wait()\n    return results\n}\n</code></pre> <p>The HNSW graph structure is read-only during search operations, which allows for safe concurrent access without locks.</p>"},{"location":"core/search-algorithms/#search-with-filters","title":"Search with Filters","text":"<p>HNSW can be extended to support filtered search, where results must satisfy certain criteria:</p> <pre><code>func (g *FilteredGraph) SearchWithFilter(query Vector, k int, filter Filter) []Node {\n    // Similar to the basic search, but with an additional filter step\n\n    // ... (code for traversing layers)\n\n    for candidates.Len() &gt; 0 {\n        current, _ := candidates.Pop()\n\n        // Only consider neighbors that satisfy the filter\n        for _, neighbor := range current.Neighbors[0] {\n            if visited[neighbor.ID] || !filter(neighbor) {\n                continue\n            }\n\n            // ... (rest of the search algorithm)\n        }\n    }\n}\n</code></pre> <p>Filtered search may require exploring more nodes to find k results that satisfy the filter, potentially increasing search time.</p>"},{"location":"core/search-algorithms/#search-with-negative-examples","title":"Search with Negative Examples","text":"<p>HNSW can be extended to support search with negative examples, where the goal is to find vectors similar to positive examples but dissimilar to negative examples:</p> <pre><code>func (g *Graph) SearchWithNegatives(positiveQuery, negativeQuery Vector, k int) []Node {\n    // ... (code for traversing layers)\n\n    for candidates.Len() &gt; 0 {\n        current, _ := candidates.Pop()\n\n        // Calculate combined distance: distance to positive - distance to negative\n        for _, neighbor := range current.Neighbors[0] {\n            if visited[neighbor.ID] {\n                continue\n            }\n\n            visited[neighbor.ID] = true\n            positiveDistance := g.Distance(positiveQuery, neighbor.Vector)\n            negativeDistance := g.Distance(negativeQuery, neighbor.Vector)\n            combinedDistance := positiveDistance - negativeDistance\n\n            // ... (rest of the search algorithm using combinedDistance)\n        }\n    }\n}\n</code></pre> <p>This approach can be useful for recommendation systems and other applications where you want to find items similar to some examples but dissimilar to others.</p>"},{"location":"core/search-algorithms/#performance-tuning","title":"Performance Tuning","text":""},{"location":"core/search-algorithms/#optimizing-ef-parameter","title":"Optimizing ef Parameter","text":"<p>The <code>ef</code> parameter has a significant impact on search performance:</p> <pre><code>// For high recall (&gt;95%)\nresults := graph.SearchWithEF(query, 10, 200)\n\n// For balanced performance\nresults := graph.SearchWithEF(query, 10, 50)\n\n// For high speed\nresults := graph.SearchWithEF(query, 10, 20)\n</code></pre> <p>You can dynamically adjust the <code>ef</code> parameter based on your requirements:</p> <pre><code>func adaptiveSearch(query Vector, k int, targetRecall float64) []Node {\n    // Start with a small ef\n    ef := k * 2\n\n    // Increase ef until we reach the target recall\n    for {\n        results := graph.SearchWithEF(query, k, ef)\n        recall := estimateRecall(results)\n\n        if recall &gt;= targetRecall {\n            return results\n        }\n\n        ef *= 2\n        if ef &gt; 1000 {\n            // Cap ef to avoid excessive computation\n            return graph.SearchWithEF(query, k, 1000)\n        }\n    }\n}\n</code></pre>"},{"location":"core/search-algorithms/#batch-search","title":"Batch Search","text":"<p>For processing multiple queries, batch search can be more efficient than individual searches:</p> <pre><code>func (g *Graph) BatchSearch(queries []Vector, k int) [][]Node {\n    results := make([][]Node, len(queries))\n\n    // Process queries in batches to utilize CPU caches better\n    batchSize := 16\n    for i := 0; i &lt; len(queries); i += batchSize {\n        end := i + batchSize\n        if end &gt; len(queries) {\n            end = len(queries)\n        }\n\n        // Process this batch concurrently\n        var wg sync.WaitGroup\n        for j := i; j &lt; end; j++ {\n            wg.Add(1)\n            go func(j int) {\n                defer wg.Done()\n                results[j] = g.Search(queries[j], k)\n            }(j)\n        }\n\n        wg.Wait()\n    }\n\n    return results\n}\n</code></pre> <p>Batch processing can improve cache locality and overall throughput.</p>"},{"location":"core/search-algorithms/#advanced-search-techniques","title":"Advanced Search Techniques","text":""},{"location":"core/search-algorithms/#hybrid-search","title":"Hybrid Search","text":"<p>HNSW can be combined with exact search methods for a hybrid approach:</p> <pre><code>func hybridSearch(query Vector, k int, exactThreshold int) []Node {\n    // Perform approximate search with HNSW\n    approximateResults := graph.Search(query, k)\n\n    // If we need high precision or have few results, perform exact search\n    if len(approximateResults) &lt; exactThreshold {\n        exactResults := exactSearch(query, k)\n        return exactResults\n    }\n\n    return approximateResults\n}\n</code></pre> <p>This approach can provide a good balance between speed and accuracy.</p>"},{"location":"core/search-algorithms/#progressive-search","title":"Progressive Search","text":"<p>For interactive applications, progressive search can provide quick initial results that improve over time:</p> <pre><code>func progressiveSearch(query Vector, k int, callback func([]Node)) {\n    // Start with a small ef for quick initial results\n    initialResults := graph.SearchWithEF(query, k, 10)\n    callback(initialResults)\n\n    // Improve results with larger ef values\n    for ef := 20; ef &lt;= 100; ef *= 2 {\n        improvedResults := graph.SearchWithEF(query, k, ef)\n        callback(improvedResults)\n    }\n}\n</code></pre> <p>This approach is useful for user interfaces where showing some results quickly is more important than waiting for the most accurate results.</p>"},{"location":"core/search-algorithms/#next-steps","title":"Next Steps","text":"<p>Now that you understand the search algorithms in HNSW, you can explore:</p> <ul> <li>Graph Structure: Learn about the hierarchical structure of HNSW</li> <li>Distance Functions: Understand how to choose the right distance function</li> <li>Performance Tuning: Learn how to optimize HNSW for your specific use case</li> </ul>"},{"location":"extensions/creating-extensions/","title":"Creating Extensions","text":"<p>HNSW is designed to be extensible, allowing you to create custom extensions that enhance its functionality. This guide will walk you through the process of creating your own extension.</p>"},{"location":"extensions/creating-extensions/#design-principles","title":"Design Principles","text":"<p>When creating an extension for HNSW, keep the following design principles in mind:</p> <ol> <li>Composition over inheritance: Extensions should wrap the core HNSW graph rather than extending it through inheritance.</li> <li>Type safety: Use Go generics to ensure type safety.</li> <li>Minimal dependencies: Extensions should have minimal dependencies on external packages.</li> <li>Performance: Extensions should be designed with performance in mind, avoiding unnecessary allocations and computations.</li> <li>Consistency: Extensions should follow the same API patterns as the core HNSW library.</li> </ol>"},{"location":"extensions/creating-extensions/#extension-structure","title":"Extension Structure","text":"<p>A typical HNSW extension consists of the following components:</p> <ol> <li>Wrapper type: A struct that wraps the core HNSW graph and adds additional functionality.</li> <li>Storage interface: An interface that defines how to store and retrieve extension-specific data.</li> <li>Storage implementation: One or more implementations of the storage interface.</li> <li>Helper functions: Functions that make it easier to use the extension.</li> </ol>"},{"location":"extensions/creating-extensions/#example-creating-a-simple-extension","title":"Example: Creating a Simple Extension","text":"<p>Let's create a simple extension that adds tagging functionality to the HNSW graph. This extension will allow you to tag nodes and search for nodes with specific tags.</p>"},{"location":"extensions/creating-extensions/#step-1-define-the-extension-package","title":"Step 1: Define the Extension Package","text":"<p>Create a new file called <code>tags.go</code> in a new package:</p> <pre><code>package tags\n\nimport (\n    \"github.com/TFMV/hnsw\"\n)\n\n// Tags represents a set of tags for a node.\ntype Tags []string\n\n// TagStore defines the interface for storing and retrieving tags.\ntype TagStore[K comparable] interface {\n    // Add adds tags for a key.\n    Add(key K, tags Tags) error\n\n    // Get retrieves tags for a key.\n    Get(key K) (Tags, error)\n\n    // Delete removes tags for a key.\n    Delete(key K) error\n\n    // Search returns keys that have all the specified tags.\n    Search(tags ...string) ([]K, error)\n}\n</code></pre>"},{"location":"extensions/creating-extensions/#step-2-implement-a-storage-backend","title":"Step 2: Implement a Storage Backend","text":"<p>Now, let's implement an in-memory storage backend for our tags:</p> <pre><code>// MemoryTagStore is an in-memory implementation of TagStore.\ntype MemoryTagStore[K comparable] struct {\n    tags map[K]Tags\n}\n\n// NewMemoryTagStore creates a new MemoryTagStore.\nfunc NewMemoryTagStore[K comparable]() *MemoryTagStore[K] {\n    return &amp;MemoryTagStore[K]{\n        tags: make(map[K]Tags),\n    }\n}\n\n// Add adds tags for a key.\nfunc (s *MemoryTagStore[K]) Add(key K, tags Tags) error {\n    s.tags[key] = tags\n    return nil\n}\n\n// Get retrieves tags for a key.\nfunc (s *MemoryTagStore[K]) Get(key K) (Tags, error) {\n    tags, ok := s.tags[key]\n    if !ok {\n        return nil, fmt.Errorf(\"no tags found for key %v\", key)\n    }\n    return tags, nil\n}\n\n// Delete removes tags for a key.\nfunc (s *MemoryTagStore[K]) Delete(key K) error {\n    delete(s.tags, key)\n    return nil\n}\n\n// Search returns keys that have all the specified tags.\nfunc (s *MemoryTagStore[K]) Search(tags ...string) ([]K, error) {\n    var result []K\n\n    for key, nodeTags := range s.tags {\n        if containsAll(nodeTags, tags) {\n            result = append(result, key)\n        }\n    }\n\n    return result, nil\n}\n\n// containsAll returns true if all items in subset are in set.\nfunc containsAll(set, subset []string) bool {\n    for _, item := range subset {\n        found := false\n        for _, setItem := range set {\n            if setItem == item {\n                found = true\n                break\n            }\n        }\n        if !found {\n            return false\n        }\n    }\n    return true\n}\n</code></pre>"},{"location":"extensions/creating-extensions/#step-3-create-a-wrapper-type","title":"Step 3: Create a Wrapper Type","text":"<p>Now, let's create a wrapper type that combines the HNSW graph with our tag store:</p> <pre><code>// TaggedGraph wraps an HNSW graph and adds tagging functionality.\ntype TaggedGraph[K comparable] struct {\n    graph    *hnsw.Graph[K]\n    tagStore TagStore[K]\n}\n\n// NewTaggedGraph creates a new TaggedGraph.\nfunc NewTaggedGraph[K comparable]() *TaggedGraph[K] {\n    return &amp;TaggedGraph[K]{\n        graph:    hnsw.NewGraph[K](),\n        tagStore: NewMemoryTagStore[K](),\n    }\n}\n\n// Add adds a node with tags to the graph.\nfunc (g *TaggedGraph[K]) Add(node hnsw.Node[K], tags Tags) error {\n    // Add the node to the graph\n    if err := g.graph.Add(node); err != nil {\n        return err\n    }\n\n    // Add the tags\n    return g.tagStore.Add(node.Key, tags)\n}\n\n// Delete removes a node and its tags from the graph.\nfunc (g *TaggedGraph[K]) Delete(key K) error {\n    // Delete the node from the graph\n    if err := g.graph.Delete(key); err != nil {\n        return err\n    }\n\n    // Delete the tags\n    return g.tagStore.Delete(key)\n}\n\n// Get retrieves a node and its tags from the graph.\nfunc (g *TaggedGraph[K]) Get(key K) (hnsw.Node[K], Tags, error) {\n    // Get the node from the graph\n    node, err := g.graph.Get(key)\n    if err != nil {\n        return hnsw.Node[K]{}, nil, err\n    }\n\n    // Get the tags\n    tags, err := g.tagStore.Get(key)\n    if err != nil {\n        return node, nil, err\n    }\n\n    return node, tags, nil\n}\n\n// Search searches for the nearest neighbors of the query vector.\nfunc (g *TaggedGraph[K]) Search(query []float32, k int) ([]hnsw.SearchResult[K], error) {\n    return g.graph.Search(query, k)\n}\n\n// SearchWithTags searches for the nearest neighbors of the query vector that have all the specified tags.\nfunc (g *TaggedGraph[K]) SearchWithTags(query []float32, k int, tags ...string) ([]hnsw.SearchResult[K], error) {\n    // Find nodes with the specified tags\n    keys, err := g.tagStore.Search(tags...)\n    if err != nil {\n        return nil, err\n    }\n\n    // If no nodes have the specified tags, return an empty result\n    if len(keys) == 0 {\n        return []hnsw.SearchResult[K]{}, nil\n    }\n\n    // Search for the nearest neighbors among the nodes with the specified tags\n    return g.graph.SearchWithFilter(query, k, func(key K) bool {\n        for _, k := range keys {\n            if k == key {\n                return true\n            }\n        }\n        return false\n    })\n}\n</code></pre>"},{"location":"extensions/creating-extensions/#step-4-add-tests","title":"Step 4: Add Tests","text":"<p>It's important to test your extension to ensure it works correctly:</p> <pre><code>package tags\n\nimport (\n    \"testing\"\n\n    \"github.com/TFMV/hnsw\"\n)\n\nfunc TestTaggedGraph(t *testing.T) {\n    // Create a new tagged graph\n    graph := NewTaggedGraph[int]()\n\n    // Add some nodes with tags\n    nodes := []hnsw.Node[int]{\n        {Key: 1, Value: []float32{0.1, 0.2, 0.3}},\n        {Key: 2, Value: []float32{0.2, 0.3, 0.4}},\n        {Key: 3, Value: []float32{0.3, 0.4, 0.5}},\n    }\n\n    tags := []Tags{\n        {\"electronics\", \"smartphone\"},\n        {\"electronics\", \"laptop\"},\n        {\"clothing\", \"shirt\"},\n    }\n\n    for i, node := range nodes {\n        err := graph.Add(node, tags[i])\n        if err != nil {\n            t.Fatalf(\"Error adding node: %v\", err)\n        }\n    }\n\n    // Test retrieving a node with tags\n    node, nodeTags, err := graph.Get(1)\n    if err != nil {\n        t.Fatalf(\"Error getting node: %v\", err)\n    }\n\n    if node.Key != 1 {\n        t.Errorf(\"Expected key 1, got %v\", node.Key)\n    }\n\n    if len(nodeTags) != 2 || nodeTags[0] != \"electronics\" || nodeTags[1] != \"smartphone\" {\n        t.Errorf(\"Expected tags [electronics smartphone], got %v\", nodeTags)\n    }\n\n    // Test searching with tags\n    query := []float32{0.15, 0.25, 0.35}\n    results, err := graph.SearchWithTags(query, 2, \"electronics\")\n    if err != nil {\n        t.Fatalf(\"Error searching with tags: %v\", err)\n    }\n\n    if len(results) != 2 {\n        t.Errorf(\"Expected 2 results, got %d\", len(results))\n    }\n\n    // The closest node should be node 1\n    if results[0].Key != 1 {\n        t.Errorf(\"Expected key 1, got %v\", results[0].Key)\n    }\n\n    // Test searching with multiple tags\n    results, err = graph.SearchWithTags(query, 2, \"electronics\", \"smartphone\")\n    if err != nil {\n        t.Fatalf(\"Error searching with tags: %v\", err)\n    }\n\n    if len(results) != 1 {\n        t.Errorf(\"Expected 1 result, got %d\", len(results))\n    }\n\n    if results[0].Key != 1 {\n        t.Errorf(\"Expected key 1, got %v\", results[0].Key)\n    }\n\n    // Test deleting a node\n    err = graph.Delete(1)\n    if err != nil {\n        t.Fatalf(\"Error deleting node: %v\", err)\n    }\n\n    // Verify that the node is deleted\n    _, _, err = graph.Get(1)\n    if err == nil {\n        t.Errorf(\"Expected error getting deleted node, got nil\")\n    }\n}\n</code></pre>"},{"location":"extensions/creating-extensions/#step-5-document-your-extension","title":"Step 5: Document Your Extension","text":"<p>Finally, document your extension to make it easier for others to use:</p> <pre><code># Tag Extension\n\nThe Tag Extension allows you to tag nodes in the HNSW graph and search for nodes with specific tags.\n\n## Features\n\n- Add tags to nodes\n- Retrieve tags for nodes\n- Search for nodes with specific tags\n- Combine tag filtering with vector search\n\n## Usage\n\n```go\n// Create a new tagged graph\ngraph := tags.NewTaggedGraph[int]()\n\n// Add a node with tags\nerr := graph.Add(hnsw.Node[int]{\n    Key:   1,\n    Value: []float32{0.1, 0.2, 0.3},\n}, tags.Tags{\"electronics\", \"smartphone\"})\n\n// Search for nodes with tags\nresults, err := graph.SearchWithTags(query, 5, \"electronics\", \"smartphone\")\n</code></pre>"},{"location":"extensions/creating-extensions/#best-practices","title":"Best Practices","text":"<ul> <li>Keep the number of tags per node reasonable</li> <li>Use specific tags for better filtering</li> <li>Consider implementing a custom tag store for very large datasets</li> </ul>"},{"location":"extensions/creating-extensions/#best-practices-for-extension-development","title":"Best Practices for Extension Development","text":"<p>When developing extensions for HNSW, follow these best practices:</p> <ol> <li>Keep it simple: Extensions should do one thing and do it well.</li> <li>Document thoroughly: Provide clear documentation and examples.</li> <li>Test extensively: Write comprehensive tests to ensure your extension works correctly.</li> <li>Consider performance: Optimize your extension for performance, especially for large datasets.</li> <li>Follow Go conventions: Follow standard Go conventions for naming, error handling, and documentation.</li> <li>Provide flexibility: Allow users to customize the behavior of your extension.</li> <li>Maintain compatibility: Ensure your extension works with the latest version of HNSW.</li> </ol>"},{"location":"extensions/creating-extensions/#contributing-extensions","title":"Contributing Extensions","text":"<p>If you've created a useful extension for HNSW, consider contributing it back to the project. This helps the community and ensures your extension is maintained alongside the core library.</p> <p>To contribute an extension:</p> <ol> <li>Fork the HNSW repository</li> <li>Add your extension to the <code>hnsw-extensions</code> directory</li> <li>Write comprehensive tests and documentation</li> <li>Submit a pull request</li> </ol>"},{"location":"extensions/creating-extensions/#next-steps","title":"Next Steps","text":"<ul> <li>Metadata Extension: Learn about the built-in Metadata Extension</li> <li>Faceted Search: Learn about the built-in Faceted Search extension</li> <li>Advanced Techniques: Learn about more advanced usage patterns</li> </ul>"},{"location":"extensions/faceted-search/","title":"Faceted Search","text":"<p>The Faceted Search extension for HNSW allows you to filter search results based on facets or attributes. This is particularly useful for applications where you need to narrow down search results based on categorical or numerical attributes, such as product categories, price ranges, or document types.</p>"},{"location":"extensions/faceted-search/#features","title":"Features","text":"<ul> <li>Filter search results based on facets (attributes)</li> <li>Support for multiple facet types (string, numeric, boolean, array)</li> <li>Efficient filtering without compromising search performance</li> <li>Combining multiple filters with AND/OR logic</li> <li>Seamless integration with the core HNSW API</li> </ul>"},{"location":"extensions/faceted-search/#installation","title":"Installation","text":"<p>The Faceted Search extension is included in the HNSW package and can be imported as follows:</p> <pre><code>import (\n    \"github.com/TFMV/hnsw\"\n    \"github.com/TFMV/hnsw/hnsw-extensions/facets\"\n)\n</code></pre>"},{"location":"extensions/faceted-search/#basic-usage","title":"Basic Usage","text":""},{"location":"extensions/faceted-search/#creating-a-faceted-graph","title":"Creating a Faceted Graph","text":"<p>To create a graph with faceted search support, use the <code>NewFacetedGraph</code> function:</p> <pre><code>// Create a new graph with faceted search support\ngraph := facets.NewFacetedGraph[int]()\n</code></pre>"},{"location":"extensions/faceted-search/#adding-vectors-with-facets","title":"Adding Vectors with Facets","text":"<p>To add a vector with facets to the graph:</p> <pre><code>// Create a vector\nvector := []float32{0.1, 0.2, 0.3, 0.4, 0.5}\n\n// Create facets\nfacetValues := map[string]interface{}{\n    \"category\":    \"electronics\",\n    \"price\":       199.99,\n    \"inStock\":     true,\n    \"tags\":        []string{\"smartphone\", \"5G\", \"camera\"},\n    \"releaseYear\": 2023,\n}\n\n// Add the vector with facets to the graph\nerr := graph.Add(hnsw.Node[int]{\n    Key:   1,\n    Value: vector,\n}, facetValues)\n</code></pre>"},{"location":"extensions/faceted-search/#searching-with-filters","title":"Searching with Filters","text":"<p>To search for vectors and filter the results based on facets:</p> <pre><code>// Create a query vector\nquery := []float32{0.15, 0.25, 0.35, 0.45, 0.55}\n\n// Create a filter\nfilter := facets.Eq(\"category\", \"electronics\")\n\n// Search for the 5 nearest neighbors that match the filter\nresults, err := graph.SearchWithFilter(query, 5, filter)\nif err != nil {\n    // Handle error\n}\n\n// Process the results\nfor _, result := range results {\n    fmt.Printf(\"Key: %d, Distance: %f\\n\", result.Key, result.Dist)\n}\n</code></pre>"},{"location":"extensions/faceted-search/#filter-types","title":"Filter Types","text":"<p>The Faceted Search extension supports several types of filters:</p>"},{"location":"extensions/faceted-search/#exact-match-filter","title":"Exact Match Filter","text":"<p>Matches facets with an exact value:</p> <pre><code>// Match products in the \"electronics\" category\nfilter := facets.Eq(\"category\", \"electronics\")\n</code></pre>"},{"location":"extensions/faceted-search/#numerical-range-filter","title":"Numerical Range Filter","text":"<p>Matches facets within a numerical range:</p> <pre><code>// Match products with a price between 100 and 300\nfilter := facets.Range(\"price\", 100.0, 300.0)\n</code></pre>"},{"location":"extensions/faceted-search/#boolean-match-filter","title":"Boolean Match Filter","text":"<p>Matches facets with a boolean value:</p> <pre><code>// Match products that are in stock\nfilter := facets.Eq(\"inStock\", true)\n</code></pre>"},{"location":"extensions/faceted-search/#array-contains-filter","title":"Array Contains Filter","text":"<p>Matches facets that contain a specific value in an array:</p> <pre><code>// Match products with the \"smartphone\" tag\nfilter := facets.Contains(\"tags\", \"smartphone\")\n</code></pre>"},{"location":"extensions/faceted-search/#combining-filters","title":"Combining Filters","text":"<p>You can combine multiple filters using AND and OR operators:</p> <pre><code>// Match products in the \"electronics\" category with a price between 100 and 300\nfilter := facets.And(\n    facets.Eq(\"category\", \"electronics\"),\n    facets.Range(\"price\", 100.0, 300.0),\n)\n\n// Match products in either the \"electronics\" or \"accessories\" category\nfilter := facets.Or(\n    facets.Eq(\"category\", \"electronics\"),\n    facets.Eq(\"category\", \"accessories\"),\n)\n\n// Complex filter: (category = \"electronics\" AND price between 100 and 300) OR (category = \"accessories\" AND inStock = true)\nfilter := facets.Or(\n    facets.And(\n        facets.Eq(\"category\", \"electronics\"),\n        facets.Range(\"price\", 100.0, 300.0),\n    ),\n    facets.And(\n        facets.Eq(\"category\", \"accessories\"),\n        facets.Eq(\"inStock\", true),\n    ),\n)\n</code></pre>"},{"location":"extensions/faceted-search/#advanced-usage","title":"Advanced Usage","text":""},{"location":"extensions/faceted-search/#batch-operations","title":"Batch Operations","text":"<p>The Faceted Search extension supports batch operations for adding and deleting nodes with facets:</p> <pre><code>// Create nodes with facets\nnodes := []hnsw.Node[int]{\n    {Key: 1, Value: []float32{0.1, 0.2, 0.3}},\n    {Key: 2, Value: []float32{0.2, 0.3, 0.4}},\n    {Key: 3, Value: []float32{0.3, 0.4, 0.5}},\n}\n\nfacets := []map[string]interface{}{\n    {\"category\": \"electronics\", \"price\": 199.99},\n    {\"category\": \"accessories\", \"price\": 29.99},\n    {\"category\": \"electronics\", \"price\": 299.99},\n}\n\n// Add the nodes with facets to the graph\nerr := graph.BatchAdd(nodes, facets)\nif err != nil {\n    // Handle error\n}\n</code></pre>"},{"location":"extensions/faceted-search/#searching-with-negative-examples","title":"Searching with Negative Examples","text":"<p>The Faceted Search extension supports searching with negative examples, which allows you to find vectors that are similar to a positive example but dissimilar to a negative example:</p> <pre><code>// Create a positive and negative query vector\npositiveQuery := []float32{0.1, 0.2, 0.3, 0.4, 0.5}\nnegativeQuery := []float32{0.5, 0.4, 0.3, 0.2, 0.1}\n\n// Create a filter\nfilter := facets.Eq(\"category\", \"electronics\")\n\n// Search for the 5 nearest neighbors that match the filter\nresults, err := graph.SearchWithNegativeAndFilter(positiveQuery, negativeQuery, 5, 0.5, filter)\nif err != nil {\n    // Handle error\n}\n</code></pre>"},{"location":"extensions/faceted-search/#custom-facet-store","title":"Custom Facet Store","text":"<p>By default, the Faceted Search extension uses an in-memory store for facets. However, you can implement your own facet store by implementing the <code>FacetStore</code> interface:</p> <pre><code>type FacetStore[K comparable] interface {\n    Add(key K, facets map[string]interface{}) error\n    Get(key K) (map[string]interface{}, error)\n    Delete(key K) error\n    Filter(filter Filter) ([]K, error)\n}\n</code></pre> <p>For example, you could implement a facet store that uses a database or a file system to store facets.</p>"},{"location":"extensions/faceted-search/#performance-considerations","title":"Performance Considerations","text":"<p>The Faceted Search extension is designed to be efficient, but filtering can add overhead to the search process. Consider the following tips:</p> <ul> <li>Keep the number of facets per node reasonable</li> <li>Use batch operations for better performance when adding or retrieving multiple nodes</li> <li>Consider implementing a custom facet store for very large datasets or when persistence is required</li> <li>Complex filters (with many AND/OR conditions) may slow down the search process</li> </ul>"},{"location":"extensions/faceted-search/#next-steps","title":"Next Steps","text":"<ul> <li>Metadata Extension: Learn how to store and retrieve metadata alongside vectors</li> <li>Creating Extensions: Learn how to create your own extensions</li> <li>Advanced Techniques: Learn about more advanced usage patterns</li> </ul>"},{"location":"extensions/metadata/","title":"Metadata Extension","text":"<p>The Metadata Extension for HNSW allows you to store and retrieve JSON metadata alongside vectors in the graph. This is useful for applications where you need to associate additional information with each vector, such as product details, document content, or image attributes.</p>"},{"location":"extensions/metadata/#features","title":"Features","text":"<ul> <li>Store arbitrary JSON metadata with each vector</li> <li>Retrieve metadata along with search results</li> <li>Type-safe implementation using Go generics</li> <li>Memory-efficient storage</li> <li>Support for batch operations</li> <li>Seamless integration with the core HNSW API</li> </ul>"},{"location":"extensions/metadata/#installation","title":"Installation","text":"<p>The Metadata Extension is included in the HNSW package and can be imported as follows:</p> <pre><code>import (\n    \"github.com/TFMV/hnsw\"\n    \"github.com/TFMV/hnsw/hnsw-extensions/meta\"\n)\n</code></pre>"},{"location":"extensions/metadata/#basic-usage","title":"Basic Usage","text":""},{"location":"extensions/metadata/#creating-a-metadata-graph","title":"Creating a Metadata Graph","text":"<p>To create a graph with metadata support, use the <code>NewMetaGraph</code> function:</p> <pre><code>// Define a type for your metadata\ntype ProductMetadata struct {\n    Name        string    `json:\"name\"`\n    Category    string    `json:\"category\"`\n    Price       float64   `json:\"price\"`\n    Tags        []string  `json:\"tags\"`\n    InStock     bool      `json:\"in_stock\"`\n    ReleaseDate time.Time `json:\"release_date\"`\n}\n\n// Create a new graph with metadata support\ngraph := meta.NewMetaGraph[int, ProductMetadata]()\n</code></pre>"},{"location":"extensions/metadata/#adding-vectors-with-metadata","title":"Adding Vectors with Metadata","text":"<p>To add a vector with metadata to the graph:</p> <pre><code>// Create a vector\nvector := []float32{0.1, 0.2, 0.3, 0.4, 0.5}\n\n// Create metadata\nmetadata := ProductMetadata{\n    Name:        \"Product 1\",\n    Category:    \"Electronics\",\n    Price:       199.99,\n    Tags:        []string{\"gadget\", \"smartphone\"},\n    InStock:     true,\n    ReleaseDate: time.Now(),\n}\n\n// Add the vector with metadata to the graph\nerr := graph.Add(hnsw.Node[int]{\n    Key:   1,\n    Value: vector,\n}, metadata)\n</code></pre>"},{"location":"extensions/metadata/#searching-and-retrieving-metadata","title":"Searching and Retrieving Metadata","text":"<p>To search for vectors and retrieve their metadata:</p> <pre><code>// Create a query vector\nquery := []float32{0.15, 0.25, 0.35, 0.45, 0.55}\n\n// Search for the 5 nearest neighbors and retrieve their metadata\nresults, err := graph.SearchWithMetadata(query, 5)\nif err != nil {\n    // Handle error\n}\n\n// Process the results\nfor _, result := range results {\n    fmt.Printf(\"Key: %d, Distance: %f\\n\", result.Key, result.Dist)\n    fmt.Printf(\"Name: %s, Price: %.2f\\n\", result.Metadata.Name, result.Metadata.Price)\n}\n</code></pre>"},{"location":"extensions/metadata/#retrieving-metadata-for-a-specific-node","title":"Retrieving Metadata for a Specific Node","text":"<p>To retrieve the metadata for a specific node:</p> <pre><code>metadata, err := graph.GetMetadata(1)\nif err != nil {\n    // Handle error\n}\nfmt.Printf(\"Name: %s, Price: %.2f\\n\", metadata.Name, metadata.Price)\n</code></pre>"},{"location":"extensions/metadata/#updating-metadata","title":"Updating Metadata","text":"<p>To update the metadata for an existing node:</p> <pre><code>// Get the existing metadata\nmetadata, err := graph.GetMetadata(1)\nif err != nil {\n    // Handle error\n}\n\n// Update the metadata\nmetadata.Price = 149.99\nmetadata.InStock = false\n\n// Save the updated metadata\nerr = graph.UpdateMetadata(1, metadata)\nif err != nil {\n    // Handle error\n}\n</code></pre>"},{"location":"extensions/metadata/#deleting-nodes-with-metadata","title":"Deleting Nodes with Metadata","text":"<p>To delete a node and its metadata:</p> <pre><code>err := graph.Delete(1)\nif err != nil {\n    // Handle error\n}\n</code></pre>"},{"location":"extensions/metadata/#batch-operations","title":"Batch Operations","text":""},{"location":"extensions/metadata/#adding-multiple-vectors-with-metadata","title":"Adding Multiple Vectors with Metadata","text":"<p>To add multiple vectors with metadata in a batch:</p> <pre><code>// Create nodes with metadata\nnodes := []hnsw.Node[int]{\n    {Key: 1, Value: []float32{0.1, 0.2, 0.3}},\n    {Key: 2, Value: []float32{0.2, 0.3, 0.4}},\n    {Key: 3, Value: []float32{0.3, 0.4, 0.5}},\n}\n\nmetadata := []ProductMetadata{\n    {Name: \"Product 1\", Price: 199.99},\n    {Name: \"Product 2\", Price: 299.99},\n    {Name: \"Product 3\", Price: 399.99},\n}\n\n// Add the nodes with metadata to the graph\nerr := graph.BatchAdd(nodes, metadata)\nif err != nil {\n    // Handle error\n}\n</code></pre>"},{"location":"extensions/metadata/#retrieving-metadata-for-multiple-nodes","title":"Retrieving Metadata for Multiple Nodes","text":"<p>To retrieve metadata for multiple nodes:</p> <pre><code>keys := []int{1, 2, 3}\nmetadataMap, err := graph.GetMetadataBatch(keys)\nif err != nil {\n    // Handle error\n}\n\nfor key, metadata := range metadataMap {\n    fmt.Printf(\"Key: %d, Name: %s, Price: %.2f\\n\", key, metadata.Name, metadata.Price)\n}\n</code></pre>"},{"location":"extensions/metadata/#advanced-usage","title":"Advanced Usage","text":""},{"location":"extensions/metadata/#custom-metadata-store","title":"Custom Metadata Store","text":"<p>By default, the Metadata Extension uses an in-memory store for metadata. However, you can implement your own metadata store by implementing the <code>MetadataStore</code> interface:</p> <pre><code>type MetadataStore[K comparable, M any] interface {\n    Add(key K, metadata M) error\n    Get(key K) (M, error)\n    GetBatch(keys []K) (map[K]M, error)\n    Update(key K, metadata M) error\n    Delete(key K) error\n    DeleteBatch(keys []K) error\n}\n</code></pre> <p>For example, you could implement a metadata store that uses a database or a file system to store metadata.</p>"},{"location":"extensions/metadata/#combining-with-other-extensions","title":"Combining with Other Extensions","text":"<p>The Metadata Extension can be combined with other extensions, such as the Faceted Search extension, to create a more powerful search system. See the Extensions Overview for more information.</p>"},{"location":"extensions/metadata/#performance-considerations","title":"Performance Considerations","text":"<p>The Metadata Extension is designed to be memory-efficient and performant. However, storing large amounts of metadata can increase memory usage and affect performance. Consider the following tips:</p> <ul> <li>Keep metadata small and focused on essential information</li> <li>Use batch operations for better performance when adding or retrieving multiple nodes</li> <li>Consider implementing a custom metadata store for very large datasets or when persistence is required</li> </ul>"},{"location":"extensions/metadata/#next-steps","title":"Next Steps","text":"<ul> <li>Faceted Search: Learn how to filter search results based on facets</li> <li>Creating Extensions: Learn how to create your own extensions</li> <li>Examples: See more examples of using the Metadata Extension</li> </ul>"},{"location":"extensions/overview/","title":"Extensions Overview","text":"<p>HNSW is designed to be extensible, allowing you to enhance its core functionality through modular extensions. This page provides an overview of the available extensions and how to use them.</p>"},{"location":"extensions/overview/#available-extensions","title":"Available Extensions","text":"<p>HNSW comes with several built-in extensions that provide additional functionality:</p>"},{"location":"extensions/overview/#metadata-extension","title":"Metadata Extension","text":"<p>The Metadata Extension allows you to store and retrieve JSON metadata alongside vectors in the HNSW graph.</p> <p>Key features:</p> <ul> <li>Type-safe implementation using Go generics</li> <li>Memory-efficient storage of metadata</li> <li>Support for batch operations</li> <li>Seamless integration with the core HNSW API</li> </ul> <p>Learn more about the Metadata Extension</p>"},{"location":"extensions/overview/#faceted-search","title":"Faceted Search","text":"<p>The Faceted Search extension enables filtering of search results based on facets (attributes) of the vectors.</p> <p>Key features:</p> <ul> <li>Support for multiple facet types (string, numeric, boolean, array)</li> <li>Efficient filtering of search results</li> <li>Combining multiple filters with AND/OR logic</li> <li>Seamless integration with the core HNSW API</li> </ul> <p>Learn more about Faceted Search</p>"},{"location":"extensions/overview/#using-extensions","title":"Using Extensions","text":"<p>To use an extension, you need to import it alongside the core HNSW package:</p> <pre><code>import (\n    \"github.com/TFMV/hnsw\"\n    \"github.com/TFMV/hnsw/hnsw-extensions/meta\"\n    \"github.com/TFMV/hnsw/hnsw-extensions/facets\"\n)\n</code></pre>"},{"location":"extensions/overview/#example-using-the-metadata-extension","title":"Example: Using the Metadata Extension","text":"<pre><code>// Create a new graph with metadata\ngraph := meta.NewMetaGraph[int, ProductMetadata]()\n\n// Add a vector with metadata\nerr := graph.Add(hnsw.Node[int]{\n    Key:   1,\n    Value: []float32{0.1, 0.2, 0.3},\n}, ProductMetadata{\n    Name:  \"Product 1\",\n    Price: 19.99,\n    Tags:  []string{\"electronics\", \"gadgets\"},\n})\n\n// Search and retrieve metadata\nresults, err := graph.SearchWithMetadata(query, 5)\nfor _, result := range results {\n    fmt.Printf(\"Product: %s, Price: %.2f\\n\", result.Metadata.Name, result.Metadata.Price)\n}\n</code></pre>"},{"location":"extensions/overview/#example-using-the-faceted-search-extension","title":"Example: Using the Faceted Search Extension","text":"<pre><code>// Create a new graph with facets\ngraph := facets.NewFacetedGraph[int]()\n\n// Add a vector with facets\nerr := graph.Add(hnsw.Node[int]{\n    Key:   1,\n    Value: []float32{0.1, 0.2, 0.3},\n}, map[string]interface{}{\n    \"category\": \"electronics\",\n    \"price\":    19.99,\n    \"inStock\":  true,\n})\n\n// Search with a filter\nfilter := facets.And(\n    facets.Eq(\"category\", \"electronics\"),\n    facets.Range(\"price\", 10.0, 50.0),\n    facets.Eq(\"inStock\", true),\n)\nresults, err := graph.SearchWithFilter(query, 5, filter)\n</code></pre>"},{"location":"extensions/overview/#combining-extensions","title":"Combining Extensions","text":"<p>You can combine multiple extensions to create a more powerful search system. For example, you can combine the Metadata Extension with the Faceted Search extension:</p> <pre><code>// Create a graph with both metadata and facets\ngraph := meta.NewMetaGraph[int, ProductMetadata]()\nfacetedGraph := facets.WrapGraph(graph)\n\n// Add a vector with both metadata and facets\nmetadata := ProductMetadata{\n    Name:  \"Product 1\",\n    Price: 19.99,\n    Tags:  []string{\"electronics\", \"gadgets\"},\n}\nfacets := map[string]interface{}{\n    \"category\": \"electronics\",\n    \"price\":    19.99,\n    \"inStock\":  true,\n}\nerr := facetedGraph.Add(hnsw.Node[int]{\n    Key:   1,\n    Value: []float32{0.1, 0.2, 0.3},\n}, metadata, facets)\n\n// Search with a filter and retrieve metadata\nfilter := facets.Eq(\"category\", \"electronics\")\nresults, err := facetedGraph.SearchWithFilterAndMetadata(query, 5, filter)\n</code></pre>"},{"location":"extensions/overview/#creating-custom-extensions","title":"Creating Custom Extensions","text":"<p>You can create your own extensions to add custom functionality to HNSW. See the Creating Extensions guide for more information.</p>"},{"location":"extensions/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Metadata Extension: Learn more about storing and retrieving metadata</li> <li>Faceted Search: Learn more about filtering search results</li> <li>Creating Extensions: Learn how to create your own extensions</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>This guide explains the various configuration options available in HNSW and how they affect performance.</p>"},{"location":"getting-started/configuration/#graph-parameters","title":"Graph Parameters","text":"<p>When creating a new HNSW graph, you can configure several parameters to optimize performance for your specific use case:</p> <pre><code>graph := hnsw.NewGraph[int]()\n\n// Configure the graph\ngraph.M = 16              // Maximum number of connections per node\ngraph.EfConstruction = 200 // Size of the dynamic candidate list during construction\ngraph.EfSearch = 100       // Size of the dynamic candidate list during search\ngraph.Distance = hnsw.CosineDistance // Distance function to use\n</code></pre>"},{"location":"getting-started/configuration/#m-maximum-connections","title":"M (Maximum Connections)","text":"<p>The <code>M</code> parameter controls the maximum number of connections per node in the graph. A higher value of <code>M</code> results in a more connected graph, which can improve search accuracy but increases memory usage and construction time.</p> <ul> <li>Default value: 16</li> <li>Recommended range: 8-64</li> <li>Effect on performance:</li> <li>Higher values improve search accuracy but increase memory usage and construction time</li> <li>Lower values reduce memory usage and construction time but may reduce search accuracy</li> </ul>"},{"location":"getting-started/configuration/#efconstruction","title":"EfConstruction","text":"<p>The <code>EfConstruction</code> parameter controls the size of the dynamic candidate list during graph construction. A higher value results in a more accurate graph but increases construction time.</p> <ul> <li>Default value: 200</li> <li>Recommended range: 100-500</li> <li>Effect on performance:</li> <li>Higher values improve graph quality but increase construction time</li> <li>Lower values reduce construction time but may reduce graph quality</li> </ul>"},{"location":"getting-started/configuration/#efsearch","title":"EfSearch","text":"<p>The <code>EfSearch</code> parameter controls the size of the dynamic candidate list during search. A higher value results in more accurate search results but increases search time.</p> <ul> <li>Default value: 100</li> <li>Recommended range: 50-200</li> <li>Effect on performance:</li> <li>Higher values improve search accuracy but increase search time</li> <li>Lower values reduce search time but may reduce search accuracy</li> </ul>"},{"location":"getting-started/configuration/#distance-functions","title":"Distance Functions","text":"<p>HNSW provides several built-in distance functions:</p> <pre><code>// Euclidean distance\ngraph.Distance = hnsw.EuclideanDistance\n\n// Cosine distance\ngraph.Distance = hnsw.CosineDistance\n\n// Dot product distance\ngraph.Distance = hnsw.DotProductDistance\n</code></pre> <p>You can also define your own distance function:</p> <pre><code>graph.Distance = func(a, b []float32) float32 {\n    // Custom distance calculation\n    var sum float32\n    for i := range a {\n        sum += (a[i] - b[i]) * (a[i] - b[i])\n    }\n    return sum\n}\n</code></pre>"},{"location":"getting-started/configuration/#performance-tuning","title":"Performance Tuning","text":"<p>Here are some tips for tuning HNSW for optimal performance:</p>"},{"location":"getting-started/configuration/#memory-vs-speed-trade-off","title":"Memory vs. Speed Trade-off","text":"<ul> <li>For faster search at the cost of more memory, increase <code>M</code> and <code>EfSearch</code></li> <li>For lower memory usage at the cost of slower search, decrease <code>M</code> and <code>EfSearch</code></li> </ul>"},{"location":"getting-started/configuration/#construction-time-vs-search-quality-trade-off","title":"Construction Time vs. Search Quality Trade-off","text":"<ul> <li>For faster construction at the cost of search quality, decrease <code>EfConstruction</code></li> <li>For better search quality at the cost of longer construction time, increase <code>EfConstruction</code></li> </ul>"},{"location":"getting-started/configuration/#batch-operations","title":"Batch Operations","text":"<p>For better performance when adding many vectors, use batch operations:</p> <pre><code>nodes := []hnsw.Node[int]{\n    {Key: 1, Value: []float32{0.1, 0.2, 0.3}},\n    {Key: 2, Value: []float32{0.2, 0.3, 0.4}},\n    {Key: 3, Value: []float32{0.3, 0.4, 0.5}},\n}\nerr := graph.BatchAdd(nodes)\n</code></pre>"},{"location":"getting-started/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"getting-started/configuration/#high-accuracy-configuration","title":"High Accuracy Configuration","text":"<pre><code>graph := hnsw.NewGraph[int]()\ngraph.M = 32\ngraph.EfConstruction = 400\ngraph.EfSearch = 200\n</code></pre>"},{"location":"getting-started/configuration/#fast-construction-configuration","title":"Fast Construction Configuration","text":"<pre><code>graph := hnsw.NewGraph[int]()\ngraph.M = 12\ngraph.EfConstruction = 100\ngraph.EfSearch = 50\n</code></pre>"},{"location":"getting-started/configuration/#balanced-configuration","title":"Balanced Configuration","text":"<pre><code>graph := hnsw.NewGraph[int]()\ngraph.M = 16\ngraph.EfConstruction = 200\ngraph.EfSearch = 100\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to configure HNSW, you can explore more features:</p> <ul> <li>Basic Usage: Learn about more basic usage patterns</li> <li>Metadata Extension: Store and retrieve metadata alongside vectors</li> <li>Faceted Search: Filter search results based on facets</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install HNSW in your Go project.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Go 1.18 or later (for generics support)</li> <li>A Go project initialized with Go modules</li> </ul>"},{"location":"getting-started/installation/#installing-the-library","title":"Installing the Library","text":"<p>You can install HNSW using the <code>go get</code> command:</p> <pre><code>go get github.com/TFMV/hnsw\n</code></pre> <p>This will download the latest version of the library and add it to your <code>go.mod</code> file.</p>"},{"location":"getting-started/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>To verify that the library is installed correctly, you can create a simple Go program that imports the library:</p> <pre><code>package main\n\nimport (\n \"fmt\"\n\n \"github.com/TFMV/hnsw\"\n)\n\nfunc main() {\n // Create a new HNSW graph\n graph := hnsw.NewGraph[int]()\n fmt.Println(\"HNSW graph created successfully!\")\n}\n</code></pre> <p>Save this file as <code>verify.go</code> and run it:</p> <pre><code>go run verify.go\n</code></pre> <p>If the installation was successful, you should see the message \"HNSW graph created successfully!\".</p>"},{"location":"getting-started/installation/#installing-extensions","title":"Installing Extensions","text":"<p>HNSW comes with several extensions that provide additional functionality. These extensions are included in the main package and don't require separate installation.</p> <p>To use the extensions, you can import them as follows:</p> <pre><code>import (\n \"github.com/TFMV/hnsw\"\n \"github.com/TFMV/hnsw/hnsw-extensions/meta\"  // For metadata extension\n \"github.com/TFMV/hnsw/hnsw-extensions/facets\" // For faceted search\n)\n</code></pre>"},{"location":"getting-started/installation/#building-from-source","title":"Building from Source","text":"<p>If you want to build HNSW from source, you can clone the repository and build it using the standard Go tools:</p> <pre><code>git clone https://github.com/TFMV/hnsw.git\ncd hnsw\ngo build ./...\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have installed HNSW, you can proceed to the Quick Start guide to learn how to use the library.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with HNSW by walking through a simple example.</p>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":"<p>Here's a complete example that demonstrates how to create a graph, add vectors, and perform a search:</p> <pre><code>package main\n\nimport (\n \"fmt\"\n\n \"github.com/TFMV/hnsw\"\n)\n\nfunc main() {\n // Create a new HNSW graph with default parameters\n graph := hnsw.NewGraph[int]()\n\n // Add some vectors to the graph\n for i := 0; i &lt; 1000; i++ {\n  // Create a vector with 10 dimensions\n  vector := make([]float32, 10)\n  for j := 0; j &lt; 10; j++ {\n   vector[j] = float32(i) * 0.01 * float32(j+1)\n  }\n\n  // Add the vector to the graph\n  node := hnsw.Node[int]{\n   Key:   i,\n   Value: vector,\n  }\n  err := graph.Add(node)\n  if err != nil {\n   fmt.Printf(\"Error adding node: %v\\n\", err)\n  }\n }\n\n // Create a query vector\n query := []float32{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}\n\n // Search for the 5 nearest neighbors\n results, err := graph.Search(query, 5)\n if err != nil {\n  fmt.Printf(\"Error searching: %v\\n\", err)\n  return\n }\n\n // Print the results\n fmt.Println(\"Search results:\")\n for i, result := range results {\n  fmt.Printf(\"%d. Key: %d, Distance: %f\\n\", i+1, result.Key, result.Dist)\n }\n}\n</code></pre>"},{"location":"getting-started/quick-start/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"getting-started/quick-start/#1-creating-a-graph","title":"1. Creating a Graph","text":"<pre><code>graph := hnsw.NewGraph[int]()\n</code></pre> <p>This creates a new HNSW graph with default parameters. The type parameter <code>int</code> specifies the type of the keys used to identify vectors in the graph.</p> <p>You can also create a graph with custom parameters:</p> <pre><code>graph := hnsw.NewGraph[int]()\ngraph.M = 16              // Maximum number of connections per node\ngraph.EfConstruction = 200 // Size of the dynamic candidate list during construction\ngraph.EfSearch = 100       // Size of the dynamic candidate list during search\ngraph.Distance = hnsw.CosineDistance // Distance function to use\n</code></pre>"},{"location":"getting-started/quick-start/#2-adding-vectors","title":"2. Adding Vectors","text":"<pre><code>node := hnsw.Node[int]{\n    Key:   i,\n    Value: vector,\n}\nerr := graph.Add(node)\n</code></pre> <p>Each vector is added to the graph as a <code>Node</code> with a unique key and a vector value. The vector is a slice of <code>float32</code> values.</p> <p>You can also add multiple vectors in a batch:</p> <pre><code>nodes := []hnsw.Node[int]{\n    {Key: 1, Value: []float32{0.1, 0.2, 0.3}},\n    {Key: 2, Value: []float32{0.2, 0.3, 0.4}},\n    {Key: 3, Value: []float32{0.3, 0.4, 0.5}},\n}\nerr := graph.BatchAdd(nodes)\n</code></pre>"},{"location":"getting-started/quick-start/#3-searching","title":"3. Searching","text":"<pre><code>results, err := graph.Search(query, 5)\n</code></pre> <p>This searches for the 5 nearest neighbors to the query vector. The results are returned as a slice of <code>SearchResult</code> structs, which contain the key of the node and the distance to the query vector.</p>"},{"location":"getting-started/quick-start/#4-using-different-distance-functions","title":"4. Using Different Distance Functions","text":"<p>HNSW provides several built-in distance functions:</p> <pre><code>// Euclidean distance\ngraph.Distance = hnsw.EuclideanDistance\n\n// Cosine distance\ngraph.Distance = hnsw.CosineDistance\n\n// Dot product distance\ngraph.Distance = hnsw.DotProductDistance\n</code></pre> <p>You can also define your own distance function:</p> <pre><code>graph.Distance = func(a, b []float32) float32 {\n    // Custom distance calculation\n    var sum float32\n    for i := range a {\n        sum += (a[i] - b[i]) * (a[i] - b[i])\n    }\n    return sum\n}\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've seen the basic usage of HNSW, you can explore more features:</p> <ul> <li>Configuration: Learn how to configure HNSW for optimal performance</li> <li>Metadata Extension: Store and retrieve metadata alongside vectors</li> <li>Faceted Search: Filter search results based on facets</li> <li>Examples: See more examples of using HNSW</li> </ul>"},{"location":"usage/basic-usage/","title":"Basic Usage","text":"<p>This guide covers the fundamental operations you can perform with HNSW, including creating graphs, adding vectors, searching, and deleting nodes.</p>"},{"location":"usage/basic-usage/#creating-a-graph","title":"Creating a Graph","text":"<p>To create a new HNSW graph with default parameters:</p> <pre><code>import \"github.com/TFMV/hnsw\"\n\n// Create a new graph with int keys\ngraph := hnsw.NewGraph[int]()\n</code></pre> <p>You can also create a graph with custom parameters:</p> <pre><code>graph := hnsw.NewGraph[int]()\n\n// Configure the graph\ngraph.M = 16              // Maximum number of connections per node\ngraph.Ml = 0.25           // Level generation factor\ngraph.EfConstruction = 200 // Size of the dynamic candidate list during construction\ngraph.EfSearch = 100       // Size of the dynamic candidate list during search\ngraph.Distance = hnsw.CosineDistance // Distance function to use\n</code></pre>"},{"location":"usage/basic-usage/#adding-vectors","title":"Adding Vectors","text":""},{"location":"usage/basic-usage/#adding-a-single-vector","title":"Adding a Single Vector","text":"<pre><code>// Create a vector\nvector := []float32{0.1, 0.2, 0.3, 0.4, 0.5}\n\n// Create a node with a unique key\nnode := hnsw.Node[int]{\n    Key:   1,\n    Value: vector,\n}\n\n// Add the node to the graph\nerr := graph.Add(node)\nif err != nil {\n    // Handle error\n    log.Fatalf(\"Failed to add node: %v\", err)\n}\n</code></pre>"},{"location":"usage/basic-usage/#adding-multiple-vectors","title":"Adding Multiple Vectors","text":"<p>For better performance when adding many vectors, use the <code>BatchAdd</code> method:</p> <pre><code>// Create multiple nodes\nnodes := []hnsw.Node[int]{\n    {Key: 1, Value: []float32{0.1, 0.2, 0.3, 0.4, 0.5}},\n    {Key: 2, Value: []float32{0.2, 0.3, 0.4, 0.5, 0.6}},\n    {Key: 3, Value: []float32{0.3, 0.4, 0.5, 0.6, 0.7}},\n}\n\n// Add the nodes to the graph in a batch\nerr := graph.BatchAdd(nodes)\nif err != nil {\n    // Handle error\n    log.Fatalf(\"Failed to add nodes: %v\", err)\n}\n</code></pre>"},{"location":"usage/basic-usage/#searching","title":"Searching","text":""},{"location":"usage/basic-usage/#basic-search","title":"Basic Search","text":"<p>To find the nearest neighbors to a query vector:</p> <pre><code>// Create a query vector\nquery := []float32{0.15, 0.25, 0.35, 0.45, 0.55}\n\n// Search for the 5 nearest neighbors\nresults, err := graph.Search(query, 5)\nif err != nil {\n    // Handle error\n    log.Fatalf(\"Failed to search: %v\", err)\n}\n\n// Process the results\nfor i, result := range results {\n    fmt.Printf(\"%d. Key: %d, Distance: %f\\n\", i+1, result.Key, result.Dist)\n}\n</code></pre>"},{"location":"usage/basic-usage/#parallel-search","title":"Parallel Search","text":"<p>For large graphs or high-dimensional data, you can use parallel search for better performance:</p> <pre><code>// Search for the 5 nearest neighbors using 4 worker goroutines\nresults, err := graph.ParallelSearch(query, 5, 4)\nif err != nil {\n    // Handle error\n    log.Fatalf(\"Failed to search: %v\", err)\n}\n</code></pre>"},{"location":"usage/basic-usage/#search-with-negative-examples","title":"Search with Negative Examples","text":"<p>To find vectors that are similar to a positive example but dissimilar to a negative example:</p> <pre><code>// Create a positive query vector\npositiveQuery := []float32{0.1, 0.2, 0.3, 0.4, 0.5}\n\n// Create a negative query vector\nnegativeQuery := []float32{0.5, 0.4, 0.3, 0.2, 0.1}\n\n// Search for the 5 nearest neighbors that are similar to the positive query\n// but dissimilar to the negative query, with a negative weight of 0.5\nresults, err := graph.SearchWithNegative(positiveQuery, negativeQuery, 5, 0.5)\nif err != nil {\n    // Handle error\n    log.Fatalf(\"Failed to search: %v\", err)\n}\n</code></pre> <p>You can also search with multiple negative examples:</p> <pre><code>// Create multiple negative query vectors\nnegativeQueries := [][]float32{\n    {0.5, 0.4, 0.3, 0.2, 0.1},\n    {0.6, 0.5, 0.4, 0.3, 0.2},\n}\n\n// Search with multiple negative examples\nresults, err := graph.SearchWithNegatives(positiveQuery, negativeQueries, 5, 0.5)\nif err != nil {\n    // Handle error\n    log.Fatalf(\"Failed to search: %v\", err)\n}\n</code></pre>"},{"location":"usage/basic-usage/#deleting-nodes","title":"Deleting Nodes","text":""},{"location":"usage/basic-usage/#deleting-a-single-node","title":"Deleting a Single Node","text":"<pre><code>// Delete a node by key\ndeleted := graph.Delete(1)\nif !deleted {\n    fmt.Println(\"Node not found\")\n}\n</code></pre>"},{"location":"usage/basic-usage/#deleting-multiple-nodes","title":"Deleting Multiple Nodes","text":"<pre><code>// Delete multiple nodes by key\nkeys := []int{1, 2, 3}\nresults := graph.BatchDelete(keys)\n\n// Check which nodes were deleted\nfor i, deleted := range results {\n    if deleted {\n        fmt.Printf(\"Node %d was deleted\\n\", keys[i])\n    } else {\n        fmt.Printf(\"Node %d was not found\\n\", keys[i])\n    }\n}\n</code></pre>"},{"location":"usage/basic-usage/#looking-up-nodes","title":"Looking Up Nodes","text":"<p>To retrieve a vector by its key:</p> <pre><code>// Look up a node by key\nvector, found := graph.Lookup(1)\nif !found {\n    fmt.Println(\"Node not found\")\n} else {\n    fmt.Printf(\"Vector: %v\\n\", vector)\n}\n</code></pre>"},{"location":"usage/basic-usage/#getting-graph-information","title":"Getting Graph Information","text":"<p>To get the number of nodes in the graph:</p> <pre><code>// Get the number of nodes in the graph\ncount := graph.Len()\nfmt.Printf(\"The graph contains %d nodes\\n\", count)\n</code></pre>"},{"location":"usage/basic-usage/#saving-and-loading-graphs","title":"Saving and Loading Graphs","text":"<p>To save a graph to a file:</p> <pre><code>// Open a file for writing\nfile, err := os.Create(\"graph.bin\")\nif err != nil {\n    log.Fatalf(\"Failed to create file: %v\", err)\n}\ndefer file.Close()\n\n// Export the graph\nerr = graph.Export(file)\nif err != nil {\n    log.Fatalf(\"Failed to export graph: %v\", err)\n}\n</code></pre> <p>To load a graph from a file:</p> <pre><code>// Open a file for reading\nfile, err := os.Open(\"graph.bin\")\nif err != nil {\n    log.Fatalf(\"Failed to open file: %v\", err)\n}\ndefer file.Close()\n\n// Create a new graph\ngraph := hnsw.NewGraph[int]()\n\n// Import the graph\nerr = graph.Import(file)\nif err != nil {\n    log.Fatalf(\"Failed to import graph: %v\", err)\n}\n</code></pre>"},{"location":"usage/basic-usage/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basic usage of HNSW, you can explore more advanced features:</p> <ul> <li>Examples: See more examples of using HNSW</li> <li>Metadata Extension: Store and retrieve metadata alongside vectors</li> <li>Faceted Search: Filter search results based on facets</li> <li>API Reference: View the complete API reference</li> </ul>"},{"location":"usage/examples/","title":"Examples","text":"<p>This page provides practical examples of using HNSW for various use cases.</p>"},{"location":"usage/examples/#text-embedding-search","title":"Text Embedding Search","text":"<p>This example demonstrates how to use HNSW to search for similar text embeddings:</p> <pre><code>package main\n\nimport (\n \"fmt\"\n \"log\"\n\n \"github.com/TFMV/hnsw\"\n)\n\nfunc main() {\n // Create a new HNSW graph\n graph := hnsw.NewGraph[string]()\n\n // Sample text embeddings (in a real application, these would be generated by a model like OpenAI's text-embedding-ada-002)\n embeddings := map[string][]float32{\n  \"dog\":     {1.0, 0.2, 0.1},\n  \"puppy\":   {0.9, 0.3, 0.2},\n  \"canine\":  {0.8, 0.3, 0.3},\n  \"cat\":     {0.1, 1.0, 0.2},\n  \"kitten\":  {0.2, 0.9, 0.3},\n  \"feline\":  {0.3, 0.8, 0.3},\n  \"bird\":    {0.1, 0.2, 1.0},\n  \"sparrow\": {0.2, 0.3, 0.9},\n  \"avian\":   {0.3, 0.3, 0.8},\n }\n\n // Add embeddings to the graph\n for text, embedding := range embeddings {\n  err := graph.Add(hnsw.Node[string]{\n   Key:   text,\n   Value: embedding,\n  })\n  if err != nil {\n   log.Fatalf(\"Failed to add node: %v\", err)\n  }\n }\n\n // Search for similar terms to \"dog\"\n results, err := graph.Search(embeddings[\"dog\"], 3)\n if err != nil {\n  log.Fatalf(\"Failed to search: %v\", err)\n }\n\n fmt.Println(\"Terms similar to 'dog':\")\n for i, result := range results {\n  fmt.Printf(\"%d. %s (distance: %.2f)\\n\", i+1, result.Key, result.Dist)\n }\n\n // Search for terms similar to \"cat\" but dissimilar to \"dog\"\n results, err = graph.SearchWithNegative(embeddings[\"cat\"], embeddings[\"dog\"], 3, 0.5)\n if err != nil {\n  log.Fatalf(\"Failed to search: %v\", err)\n }\n\n fmt.Println(\"\\nTerms similar to 'cat' but dissimilar to 'dog':\")\n for i, result := range results {\n  fmt.Printf(\"%d. %s (distance: %.2f)\\n\", i+1, result.Key, result.Dist)\n }\n}\n</code></pre>"},{"location":"usage/examples/#image-similarity-search","title":"Image Similarity Search","text":"<p>This example shows how to use HNSW for image similarity search:</p> <pre><code>package main\n\nimport (\n \"fmt\"\n \"log\"\n \"math/rand\"\n \"time\"\n\n \"github.com/TFMV/hnsw\"\n \"github.com/TFMV/hnsw/hnsw-extensions/meta\"\n)\n\n// ImageMetadata represents metadata for an image\ntype ImageMetadata struct {\n Filename  string   `json:\"filename\"`\n Width     int      `json:\"width\"`\n Height    int      `json:\"height\"`\n Format    string   `json:\"format\"`\n Tags      []string `json:\"tags\"`\n CreatedAt string   `json:\"created_at\"`\n}\n\nfunc main() {\n // Create a new metadata graph\n graph := meta.NewMetaGraph[int, ImageMetadata]()\n\n // In a real application, these would be feature vectors extracted from images\n // using a model like ResNet or EfficientNet\n rand.Seed(time.Now().UnixNano())\n\n // Add 1000 random image vectors with metadata\n for i := 0; i &lt; 1000; i++ {\n  // Create a random 128-dimensional vector\n  vector := make([]float32, 128)\n  for j := range vector {\n   vector[j] = rand.Float32()\n  }\n\n  // Normalize the vector (important for cosine distance)\n  var sum float32\n  for _, v := range vector {\n   sum += v * v\n  }\n  norm := float32(1.0 / float32(math.Sqrt(float64(sum))))\n  for j := range vector {\n   vector[j] *= norm\n  }\n\n  // Create metadata\n  metadata := ImageMetadata{\n   Filename:  fmt.Sprintf(\"image_%d.jpg\", i),\n   Width:     rand.Intn(800) + 400,\n   Height:    rand.Intn(600) + 300,\n   Format:    \"JPEG\",\n   Tags:      []string{\"sample\", fmt.Sprintf(\"category_%d\", i%10)},\n   CreatedAt: time.Now().Format(time.RFC3339),\n  }\n\n  // Add to graph\n  err := graph.Add(hnsw.Node[int]{\n   Key:   i,\n   Value: vector,\n  }, metadata)\n\n  if err != nil {\n   log.Fatalf(\"Failed to add node: %v\", err)\n  }\n }\n\n // Create a query vector (in a real application, this would be from a query image)\n queryVector := make([]float32, 128)\n for i := range queryVector {\n  queryVector[i] = rand.Float32()\n }\n\n // Normalize the query vector\n var sum float32\n for _, v := range queryVector {\n  sum += v * v\n }\n norm := float32(1.0 / float32(math.Sqrt(float64(sum))))\n for i := range queryVector {\n  queryVector[i] *= norm\n }\n\n // Search for similar images\n results, err := graph.SearchWithMetadata(queryVector, 5)\n if err != nil {\n  log.Fatalf(\"Failed to search: %v\", err)\n }\n\n fmt.Println(\"Similar images:\")\n for i, result := range results {\n  fmt.Printf(\"%d. %s (distance: %.4f)\\n\", \n   i+1, \n   result.Metadata.Filename, \n   result.Dist)\n  fmt.Printf(\"   Tags: %v\\n\", result.Metadata.Tags)\n  fmt.Printf(\"   Dimensions: %dx%d\\n\", result.Metadata.Width, result.Metadata.Height)\n }\n}\n</code></pre>"},{"location":"usage/examples/#product-recommendation","title":"Product Recommendation","text":"<p>This example demonstrates how to use HNSW with the Faceted Search extension for product recommendations:</p> <pre><code>package main\n\nimport (\n \"fmt\"\n \"log\"\n \"math/rand\"\n \"time\"\n\n \"github.com/TFMV/hnsw\"\n \"github.com/TFMV/hnsw/hnsw-extensions/facets\"\n)\n\nfunc main() {\n // Create a new faceted graph\n graph := facets.NewFacetedGraph[int]()\n\n // Sample product categories\n categories := []string{\"Electronics\", \"Clothing\", \"Home\", \"Books\", \"Sports\"}\n\n // Sample price ranges\n priceRanges := []struct{ min, max float64 }{\n  {10, 50}, {50, 100}, {100, 200}, {200, 500}, {500, 1000},\n }\n\n rand.Seed(time.Now().UnixNano())\n\n // Add 1000 random product vectors with facets\n for i := 0; i &lt; 1000; i++ {\n  // Create a random 64-dimensional vector\n  vector := make([]float32, 64)\n  for j := range vector {\n   vector[j] = rand.Float32()\n  }\n\n  // Create facets\n  category := categories[rand.Intn(len(categories))]\n  priceRange := priceRanges[rand.Intn(len(priceRanges))]\n  price := priceRange.min + rand.Float64()*(priceRange.max-priceRange.min)\n  inStock := rand.Intn(10) &lt; 8 // 80% chance of being in stock\n\n  facetValues := map[string]interface{}{\n   \"category\": category,\n   \"price\":    price,\n   \"inStock\":  inStock,\n   \"rating\":   float64(3 + rand.Intn(3)), // Rating between 3-5\n  }\n\n  // Add to graph\n  err := graph.Add(hnsw.Node[int]{\n   Key:   i,\n   Value: vector,\n  }, facetValues)\n\n  if err != nil {\n   log.Fatalf(\"Failed to add node: %v\", err)\n  }\n }\n\n // Create a query vector (in a real application, this would be from user preferences)\n queryVector := make([]float32, 64)\n for i := range queryVector {\n  queryVector[i] = rand.Float32()\n }\n\n // Create a filter for electronics products in stock with price &lt; 200\n filter := facets.And(\n  facets.Eq(\"category\", \"Electronics\"),\n  facets.Range(\"price\", 0, 200),\n  facets.Eq(\"inStock\", true),\n )\n\n // Search for similar products with filter\n results, err := graph.SearchWithFilter(queryVector, 5, filter)\n if err != nil {\n  log.Fatalf(\"Failed to search: %v\", err)\n }\n\n fmt.Println(\"Recommended Electronics products under $200:\")\n for i, result := range results {\n  // Get facets for the result\n  facets, err := graph.GetFacets(result.Key)\n  if err != nil {\n   log.Fatalf(\"Failed to get facets: %v\", err)\n  }\n\n  fmt.Printf(\"%d. Product ID: %d (similarity: %.4f)\\n\", \n   i+1, \n   result.Key, \n   1-result.Dist) // Convert distance to similarity\n  fmt.Printf(\"   Category: %s\\n\", facets[\"category\"])\n  fmt.Printf(\"   Price: $%.2f\\n\", facets[\"price\"])\n  fmt.Printf(\"   Rating: %.1f\\n\", facets[\"rating\"])\n }\n}\n</code></pre>"},{"location":"usage/examples/#concurrent-operations","title":"Concurrent Operations","text":"<p>This example demonstrates how to use HNSW in a concurrent environment:</p> <pre><code>package main\n\nimport (\n \"fmt\"\n \"log\"\n \"math/rand\"\n \"sync\"\n \"time\"\n\n \"github.com/TFMV/hnsw\"\n)\n\nfunc main() {\n // Create a new HNSW graph\n graph := hnsw.NewGraph[int]()\n\n // Add some initial nodes\n for i := 0; i &lt; 1000; i++ {\n  vector := make([]float32, 32)\n  for j := range vector {\n   vector[j] = rand.Float32()\n  }\n\n  err := graph.Add(hnsw.Node[int]{\n   Key:   i,\n   Value: vector,\n  })\n\n  if err != nil {\n   log.Fatalf(\"Failed to add node: %v\", err)\n  }\n }\n\n // Number of concurrent operations\n numOperations := 100\n\n // Wait group to wait for all goroutines to finish\n var wg sync.WaitGroup\n wg.Add(numOperations)\n\n // Perform concurrent searches\n for i := 0; i &lt; numOperations; i++ {\n  go func(id int) {\n   defer wg.Done()\n\n   // Create a random query vector\n   query := make([]float32, 32)\n   for j := range query {\n    query[j] = rand.Float32()\n   }\n\n   // Search for similar vectors\n   results, err := graph.Search(query, 5)\n   if err != nil {\n    log.Printf(\"Search error in goroutine %d: %v\", id, err)\n    return\n   }\n\n   fmt.Printf(\"Goroutine %d found %d results\\n\", id, len(results))\n  }(i)\n }\n\n // Wait for all searches to complete\n wg.Wait()\n\n // Perform concurrent adds and deletes\n wg.Add(numOperations * 2)\n\n // Concurrent adds\n for i := 0; i &lt; numOperations; i++ {\n  go func(id int) {\n   defer wg.Done()\n\n   // Create a random vector\n   vector := make([]float32, 32)\n   for j := range vector {\n    vector[j] = rand.Float32()\n   }\n\n   // Add to graph\n   nodeID := 1000 + id\n   err := graph.Add(hnsw.Node[int]{\n    Key:   nodeID,\n    Value: vector,\n   })\n\n   if err != nil {\n    log.Printf(\"Add error in goroutine %d: %v\", id, err)\n   }\n  }(i)\n }\n\n // Concurrent deletes\n for i := 0; i &lt; numOperations; i++ {\n  go func(id int) {\n   defer wg.Done()\n\n   // Delete a random node\n   nodeID := rand.Intn(1000)\n   deleted := graph.Delete(nodeID)\n\n   if deleted {\n    fmt.Printf(\"Goroutine %d deleted node %d\\n\", id, nodeID)\n   }\n  }(i)\n }\n\n // Wait for all operations to complete\n wg.Wait()\n\n fmt.Printf(\"Final graph size: %d nodes\\n\", graph.Len())\n}\n</code></pre>"},{"location":"usage/examples/#next-steps","title":"Next Steps","text":"<p>Now that you've seen some examples of using HNSW, you can explore more advanced features:</p> <ul> <li>Basic Usage: Learn more about the basic operations</li> <li>Metadata Extension: Store and retrieve metadata alongside vectors</li> <li>Faceted Search: Filter search results based on facets</li> <li>API Reference: View the complete API reference</li> </ul>"}]}