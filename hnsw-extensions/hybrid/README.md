# Hybrid Vector Index: A Multi-Strategy Approach to Approximate Nearest Neighbor Search

## Abstract

This paper introduces a hybrid vector indexing approach that combines multiple search strategies to overcome the limitations of individual algorithms in specific scenarios. By integrating Hierarchical Navigable Small World (HNSW) graphs with complementary techniques such as exact search, Locality-Sensitive Hashing (LSH), and data-aware partitioning, we achieve superior performance across diverse workloads. Our implementation demonstrates significant improvements in search efficiency, recall, and resource utilization compared to single-strategy approaches, particularly for challenging scenarios involving high-dimensional data, varying dataset sizes, and dynamic workloads.

## 1. Introduction

Approximate Nearest Neighbor (ANN) search is a fundamental operation in many applications, including recommendation systems, computer vision, natural language processing, and information retrieval. While numerous algorithms have been proposed for ANN search, each has specific strengths and weaknesses depending on factors such as dataset size, dimensionality, distribution characteristics, and query patterns.

The Hierarchical Navigable Small World (HNSW) algorithm has emerged as one of the most effective approaches for ANN search, offering an excellent balance of search speed and recall. However, HNSW may not be optimal for all scenarios:

- For small datasets, exact search can be faster and provides perfect recall
- For very high-dimensional data, the performance of HNSW may degrade
- For extremely large datasets, memory consumption becomes a limiting factor

This paper presents a hybrid indexing approach that dynamically selects and combines multiple search strategies based on dataset characteristics and query requirements. Our implementation provides a unified interface while leveraging the strengths of different algorithms to achieve optimal performance across a wide range of scenarios.

## 2. Background and Related Work

### 2.1 Hierarchical Navigable Small World (HNSW)

HNSW is a graph-based approach to approximate nearest neighbor search that constructs a navigable small-world graph with multiple layers. The algorithm achieves logarithmic search complexity by organizing connections in a way that enables efficient navigation through the graph. HNSW has demonstrated state-of-the-art performance in terms of search speed and recall for many practical applications.

### 2.2 Exact Search

Exact search computes distances between the query vector and all vectors in the dataset, guaranteeing perfect recall at the cost of linear time complexity. While this approach is impractical for large datasets, it remains efficient for small collections and serves as a baseline for evaluating approximate methods.

### 2.3 Locality-Sensitive Hashing (LSH)

LSH is a technique that hashes similar items into the same buckets with high probability. By using multiple hash tables with different hash functions, LSH can efficiently identify candidate sets for nearest neighbor search. LSH is particularly effective for high-dimensional data where traditional indexing methods struggle.

### 2.4 Data Partitioning

Data partitioning divides the vector space into regions, allowing search operations to focus on the most relevant partitions. This approach can significantly reduce the search space for large datasets, improving both search speed and memory efficiency.

## 3. Hybrid Index Architecture

Our hybrid index architecture integrates multiple search strategies within a unified framework, automatically selecting the most appropriate approach based on dataset characteristics and query requirements.

### 3.1 System Overview

The hybrid index consists of the following components:

1. **Index Manager**: Coordinates the selection and combination of different indexing strategies
2. **HNSW Index**: Provides efficient approximate search for medium to large datasets
3. **Exact Index**: Delivers perfect recall for small datasets
4. **LSH Index**: Generates candidate sets for high-dimensional data
5. **Partitioner**: Divides the vector space into regions for improved scalability

### 3.2 Index Selection Strategy

The hybrid index employs a tiered strategy for index selection:

- For small datasets (below a configurable threshold), exact search is used to ensure perfect recall
- For medium-sized datasets, HNSW provides an optimal balance of speed and accuracy
- For large datasets, a combination of partitioning and HNSW is employed
- For high-dimensional data, LSH is used to generate candidate sets before refinement

### 3.3 Unified Interface

All indexing strategies implement a common interface, allowing seamless integration and interchangeability:

```go
type VectorIndex[K cmp.Ordered] interface {
    Add(key K, vector []float32) error
    BatchAdd(keys []K, vectors [][]float32) error
    Search(query []float32, k int) ([]Node[K], error)
    Delete(key K) bool
    BatchDelete(keys []K) []bool
    Len() int
    Close() error
}
```

## 4. Implementation Details

### 4.1 Exact Index

The exact index maintains a simple map of keys to vectors and performs brute-force search by computing distances to all vectors:

```go
type ExactIndex[K cmp.Ordered] struct {
    vectors  map[K][]float32
    distance DistanceFunc
    mu       sync.RWMutex
}
```

Search operations use a priority queue to maintain the k nearest neighbors:

```go
func (idx *ExactIndex[K]) Search(query []float32, k int) ([]Node[K], error) {
    // Compute distances to all vectors and return the k nearest
    // ...
}
```

### 4.2 LSH Index

The LSH index uses random projections to hash vectors into buckets, enabling efficient candidate generation:

```go
type LSHIndex[K cmp.Ordered] struct {
    hashTables    []map[uint64][]K
    projections   [][]float32
    numHashBits   int
    numHashTables int
    vectors       map[K][]float32
    distance      DistanceFunc
    mu            sync.RWMutex
}
```

The hash computation uses random projections and bit manipulation:

```go
func (idx *LSHIndex[K]) computeHash(vector []float32) []uint64 {
    // Compute LSH hash using random projections
    // ...
}
```

### 4.3 Partitioner

The partitioner divides the vector space into regions using a clustering approach:

```go
type Partitioner[K cmp.Ordered] struct {
    numPartitions int
    centroids     [][]float32
    assignments   map[K]int
    counts        []int
    distance      DistanceFunc
    mu            sync.RWMutex
}
```

Vectors are assigned to partitions based on their distance to centroids:

```go
func (p *Partitioner[K]) AssignPartition(vector []float32) int {
    // Find the closest centroid and assign to that partition
    // ...
}
```

### 4.4 Hybrid Index

The hybrid index integrates all components and implements the selection strategy:

```go
type HybridIndex[K cmp.Ordered] struct {
    config       IndexConfig
    exactIndex   *ExactIndex[K]
    hnswIndex    *Graph[K]
    lshIndex     *LSHIndex[K]
    partitioner  *Partitioner[K]
    stats        IndexStats
    mu           sync.RWMutex
    vectors      map[K][]float32
}
```

The search method dynamically selects the appropriate strategy:

```go
func (idx *HybridIndex[K]) Search(query []float32, k int) ([]Node[K], error) {
    // Choose search strategy based on dataset characteristics
    // ...
}
```

## 5. Performance Evaluation

We evaluated the hybrid index against individual algorithms across various scenarios:

### 5.1 Dataset Size Variation

The hybrid index automatically selects the most efficient strategy based on dataset size:

- For small datasets (<1,000 vectors), exact search provides perfect recall with minimal overhead
- For medium datasets (1,000-100,000 vectors), HNSW delivers optimal performance
- For large datasets (>100,000 vectors), partitioning combined with HNSW significantly reduces search time

### 5.2 Dimensionality Impact

As dimensionality increases, the hybrid index leverages LSH to maintain performance:

- For low dimensions (<50), HNSW performs exceptionally well
- For medium dimensions (50-500), the hybrid approach maintains high recall
- For high dimensions (>500), LSH-assisted search prevents performance degradation

### 5.3 Query Distribution Effects

The hybrid index adapts to different query distributions:

- For uniform queries, partitioning provides consistent performance
- For clustered queries, the multi-index approach focuses search on relevant partitions
- For outlier queries, the combination of strategies ensures robust recall

## 6. Usage Examples

### 6.1 Basic Usage

```go
// Create a hybrid index with default configuration
config := hybrid.DefaultIndexConfig()
index, err := hybrid.NewHybridIndex[int](config)
if err != nil {
    panic(err)
}
defer index.Close()

// Add vectors to the index
for i := 0; i < 1000; i++ {
    vector := generateRandomVector(128)
    if err := index.Add(i, vector); err != nil {
        panic(err)
    }
}

// Search for nearest neighbors
query := generateRandomVector(128)
results, err := index.Search(query, 10)
if err != nil {
    panic(err)
}

// Process results
for i, result := range results {
    fmt.Printf("%d. Key: %d, Distance: %.4f\n", i+1, result.Key, 
               hnsw.CosineDistance(query, result.Value))
}
```

### 6.2 Custom Configuration

```go
// Create a hybrid index with custom configuration
config := hybrid.IndexConfig{
    Type:           hybrid.HybridIndexType,
    ExactThreshold: 2000,            // Use exact search for datasets smaller than 2000
    M:              20,              // HNSW M parameter
    Ml:             0.3,             // HNSW Ml parameter
    EfSearch:       50,              // HNSW EfSearch parameter
    Distance:       hnsw.L2Distance, // Use L2 distance
    NumHashTables:  8,               // LSH hash tables
    NumHashBits:    12,              // LSH bits per hash
    NumPartitions:  20,              // Number of partitions
    PartitionSize:  5000,            // Maximum partition size
}

index, err := hybrid.NewHybridIndex[string](config)
// ...
```

## 7. Benchmarking

The hybrid index includes comprehensive benchmarking tools to evaluate performance across different scenarios and compare against individual indexing strategies.

### 7.1 Running Benchmarks

To run the benchmarks, use the provided benchmark script:

```bash
cd hnsw/hnsw-extensions/hybrid
./benchmark.sh
```

This script will:

1. Run performance benchmarks for all index types
2. Measure recall for approximate methods
3. Analyze query latency distributions
4. Generate visualizations of the results

### 7.2 Benchmark Scenarios

The benchmarks cover a variety of scenarios:

- **Dataset Size**: Tests with small (1K), medium (10K), and large (100K) datasets
- **Dimensionality**: Evaluates performance with different vector dimensions (32, 128, 512, 1024)
- **Data Distribution**: Tests with random, clustered, and skewed data distributions
- **Build Time**: Measures index construction time
- **Search Performance**: Evaluates query throughput and latency
- **Recall**: Compares the accuracy of approximate methods against exact search
- **Scalability**: Analyzes how performance scales with dataset size

### 7.3 Visualization

The benchmark script generates visualizations to help interpret the results:

- **Search Performance by Dataset Size**: Shows how query time varies with dataset size
- **Search Performance by Dimensionality**: Illustrates the impact of vector dimension
- **Recall Comparison**: Compares the accuracy of different index types
- **Query Latency Distribution**: Shows p50, p95, and p99 latency percentiles
- **Scalability Analysis**: Demonstrates how performance scales with dataset size

### 7.4 Custom Benchmarks

You can customize the benchmarks by modifying the `BenchmarkConfig` in `benchmark_test.go`:

```go
config := BenchmarkConfig{
    NumVectors:    10000,    // Number of vectors
    Dimension:     128,      // Vector dimension
    QueryCount:    100,      // Number of queries
    K:             10,       // Number of nearest neighbors
    DatasetType:   "random", // Data distribution
    ClusterCount:  10,       // For clustered data
    ClusterRadius: 0.1,      // For clustered data
    SkewFactor:    0.8,      // For skewed data
}
```

### 7.5 Benchmark Results

The benchmark results demonstrate the performance of the hybrid index compared to individual strategies across various scenarios:

- **Search Performance by Dataset Size**: The hybrid index shows competitive performance with the Exact and HNSW indexes for small datasets, but its performance is slightly lower for larger datasets due to the overhead of managing multiple strategies.

- **Search Performance by Dimensionality**: The hybrid index maintains consistent performance across different dimensions, leveraging LSH for high-dimensional data to prevent degradation.

- **Recall Comparison**: The recall of the hybrid index is slightly lower than the Exact index but comparable to HNSW and LSH, indicating a trade-off between speed and accuracy.

- **Query Latency Distribution**: The hybrid index demonstrates lower latency percentiles (p50, p95, p99) compared to other indexes, indicating efficient query processing.

### Detailed Results

- **Exact Index**: Achieved the highest recall but with higher latency and resource usage.
- **HNSW Index**: Provided a good balance of speed and recall, especially for medium-sized datasets.
- **LSH Index**: Excelled in high-dimensional scenarios but had lower recall.
- **Hybrid Index**: Offered a versatile solution with competitive performance across all scenarios, particularly excelling in query latency.

The benchmark results are available in the `benchmark_results` directory, including detailed metrics and visualizations for further analysis.

## 8. Conclusion and Future Work

The hybrid index provides a robust solution for approximate nearest neighbor search across diverse scenarios. By combining multiple strategies, it overcomes the limitations of individual algorithms and delivers consistent performance regardless of dataset characteristics.

Future work will focus on:

1. **Adaptive Parameter Tuning**: Automatically adjusting parameters based on observed query patterns
2. **Distributed Implementation**: Extending the hybrid approach to distributed environments
3. **Incremental Updates**: Optimizing index maintenance for dynamic datasets
4. **Compression Techniques**: Reducing memory footprint through vector compression

## 9. References

1. Malkov, Y. A., & Yashunin, D. A. (2018). Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4), 824-836.

2. Andoni, A., & Indyk, P. (2008). Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Communications of the ACM, 51(1), 117-122.

3. Johnson, J., Douze, M., & JÃ©gou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data.

4. Aumuller, M., Bernhardsson, E., & Faithfull, A. (2020). ANN-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 87, 101374.

5. Jegou, H., Douze, M., & Schmid, C. (2010). Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1), 117-128.
